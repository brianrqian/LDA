{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/online_lda.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n",
      "[nltk_data] Downloading package wordnet to /Users/brian/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import dimcli\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "import spacy\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "np.random.seed(123)\n",
    "import pickle\n",
    "nltk.download('wordnet')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_hdsi_faculty_updated.csv')\n",
    "authors = df[['authors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#authors.loc[row][list_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\\'affiliations\\': [], \\'corresponding\\': \\'\\', \\'current_organization_id\\': \\'\\', \\'first_name\\': \\'Chen\\', \\'last_name\\': \\'Cai\\', \\'orcid\\': \\'\\', \\'raw_affiliation\\': [], \\'researcher_id\\': \\'\\'}, {\\'affiliations\\': [], \\'corresponding\\': \\'\\', \\'current_organization_id\\': \\'grid.261331.4\\', \\'first_name\\': \\'Woojin\\', \\'last_name\\': \\'Kim\\', \\'orcid\\': \"[\\'0000-0001-8081-5872\\']\", \\'raw_affiliation\\': [], \\'researcher_id\\': \\'ur.07705025375.34\\'}, {\\'affiliations\\': [], \\'corresponding\\': \\'\\', \\'current_organization_id\\': \\'grid.261331.4\\', \\'first_name\\': \\'Facundo\\', \\'last_name\\': \\'Mémoli\\', \\'orcid\\': \\'\\', \\'raw_affiliation\\': [], \\'researcher_id\\': \\'ur.0760761423.71\\'}, {\\'affiliations\\': [], \\'corresponding\\': \\'\\', \\'current_organization_id\\': \\'grid.261331.4\\', \\'first_name\\': \\'Yusu\\', \\'last_name\\': \\'Wang\\', \\'orcid\\': \"[\\'0000-0002-8322-4899\\']\", \\'raw_affiliation\\': [], \\'researcher_id\\': \\'ur.01357524473.18\\'}]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = str(authors.loc[0][0])\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chen Cai\n",
      "\n",
      "Woojin Kim\n",
      "ur.07705025375.34\n",
      "Facundo Mémoli\n",
      "ur.0760761423.71\n",
      "Yusu Wang\n",
      "ur.01357524473.18\n"
     ]
    }
   ],
   "source": [
    "fg = list(eval(test))#[0]['first_name']\n",
    "lis = []\n",
    "lis2 = []\n",
    "for i in fg:\n",
    "    if 'first_name' in i:\n",
    "        first = i['first_name']\n",
    "        last = i['last_name']\n",
    "        full = first + \" \" + last\n",
    "        print(full)\n",
    "        lis.append(full)\n",
    "        ids = i['researcher_id']\n",
    "        print(ids)\n",
    "        lis2.append(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new list to collect names\n",
    "new = []\n",
    "#new list to collect corresponding ids\n",
    "new2 = []\n",
    "#looping through length of author column\n",
    "for i in range(len(authors)):\n",
    "    #turning string of list of dictionaries into list of dictionaries\n",
    "    temp = list(eval(authors.loc[i][0]))\n",
    "    #names\n",
    "    lis = []\n",
    "    #ids\n",
    "    lis2 = []\n",
    "    #looping through the list of dictionaries\n",
    "    for i in temp:\n",
    "        if 'first_name' in i:\n",
    "            first = i['first_name']\n",
    "            last = i['last_name']\n",
    "            #concatenating first and last name\n",
    "            full = first + \" \" + last\n",
    "            lis.append(full)\n",
    "            #print(lis)\n",
    "            ids = i['researcher_id']\n",
    "            lis2.append(ids)\n",
    "        else:\n",
    "            lis.append(i)\n",
    "            lis2.append(i)\n",
    "    new.append(lis)\n",
    "    new2.append(lis2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding new column, \"names,\" to the original dataframe\n",
    "names = pd.Series(new)\n",
    "df['names'] = names.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding new column, \"ids,\" to the original dataframe\n",
    "ids = pd.Series(new2)\n",
    "df['ids'] = ids.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate data by researcher-year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2 = df.explode(['names', 'ids']).reset_index(drop=True)\n",
    "df2 = df.apply(pd.Series.explode).reset_index(drop=True)\n",
    "\n",
    "testing = df2['ids'].value_counts()\n",
    "#print(testing.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdsi = pd.read_csv(\"HDSI.csv\")\n",
    "faculty = hdsi[hdsi['Dimensions ID'] != 'no ID']['Dimensions ID']\n",
    "#manually adding professors since they do not have dimensions ids\n",
    "add = pd.Series(['Aaron McMillan Fraenkel', 'Justin Eldridge'])\n",
    "faculty = list(faculty.append(add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned out all names & ids that do not match our hdsi faculty list\n",
    "df3 = df2[df2.ids.isin(faculty)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>authors_count</th>\n",
       "      <th>category_for</th>\n",
       "      <th>concepts</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>journal.id</th>\n",
       "      <th>journal.title</th>\n",
       "      <th>times_cited</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>names</th>\n",
       "      <th>ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'affiliations': [], 'corresponding': '', 'cu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['space', 'metric spaces']</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>pub.1140323831</td>\n",
       "      <td>jour.1292375</td>\n",
       "      <td>SIAM Journal on Applied Algebra and Geometry</td>\n",
       "      <td>0</td>\n",
       "      <td>Elder-Rule-Staircodes for Augmented Metric Spaces</td>\n",
       "      <td>2021</td>\n",
       "      <td>Yusu Wang</td>\n",
       "      <td>ur.01357524473.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Understanding of neuronal circuitry at cellula...</td>\n",
       "      <td>[{'affiliations': [{'city': 'Cold Spring Harbo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'id': '2208', 'name': '08 Information and Co...</td>\n",
       "      <td>['hybrid architecture', 'semantic segmentation...</td>\n",
       "      <td>2020-09-28</td>\n",
       "      <td>pub.1131237716</td>\n",
       "      <td>jour.1336255</td>\n",
       "      <td>Nature Machine Intelligence</td>\n",
       "      <td>3</td>\n",
       "      <td>Semantic segmentation of microscopic neuroanat...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Yusu Wang</td>\n",
       "      <td>ur.01357524473.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>We study Vietoris–Rips complexes of metric wed...</td>\n",
       "      <td>[{'affiliations': [{'name': 'MOSEK ApS, Copenh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'id': '2201', 'name': '01 Mathematical Scien...</td>\n",
       "      <td>['Vietoris–Rips complexes', 'wedge sum', 'metr...</td>\n",
       "      <td>2020-05-20</td>\n",
       "      <td>pub.1127764757</td>\n",
       "      <td>jour.1290431</td>\n",
       "      <td>Journal of Applied and Computational Topology</td>\n",
       "      <td>5</td>\n",
       "      <td>On homotopy types of Vietoris–Rips complexes o...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Yusu Wang</td>\n",
       "      <td>ur.01357524473.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neuroscientific data analysis has traditionall...</td>\n",
       "      <td>[{'affiliations': [{'city': 'Columbus', 'city_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'id': '3120', 'name': '1109 Neurosciences'},...</td>\n",
       "      <td>['collection of neurons', 'hand-tuned paramete...</td>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>pub.1125823510</td>\n",
       "      <td>jour.1293558</td>\n",
       "      <td>bioRxiv</td>\n",
       "      <td>0</td>\n",
       "      <td>Detection and skeletonization of single neuron...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Yusu Wang</td>\n",
       "      <td>ur.01357524473.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Neuroscientific data analysis has traditionall...</td>\n",
       "      <td>[{'affiliations': [], 'corresponding': '', 'cu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'id': '3120', 'name': '1109 Neurosciences'},...</td>\n",
       "      <td>['collection of neurons', 'hand-tuned paramete...</td>\n",
       "      <td>2020-03-20</td>\n",
       "      <td>pub.1126276621</td>\n",
       "      <td>jour.1371339</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>0</td>\n",
       "      <td>Detection and skeletonization of single neuron...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Yusu Wang</td>\n",
       "      <td>ur.01357524473.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2242</th>\n",
       "      <td>20179</td>\n",
       "      <td>2203</td>\n",
       "      <td>2963037546</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['weyl', 'algorithm', 'typical', 'davis', 'unp...</td>\n",
       "      <td>['Justin Eldridge', 'Mikhail Belkin', 'Yusu Wa...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Perturbation theory', 'Eigenvalues and eigen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>Unperturbed: spectral analysis beyond Davis-Kahan</td>\n",
       "      <td>2018</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2243</th>\n",
       "      <td>20182</td>\n",
       "      <td>2204</td>\n",
       "      <td>2963918728</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['correct', 'imply', 'single', 'nesting', 'two...</td>\n",
       "      <td>['Justin Eldridge', 'Mikhail Belkin', 'Yusu Wa...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Hierarchical clustering', 'Cluster analysis'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>Beyond Hartigan Consistency: Merge Distortion ...</td>\n",
       "      <td>2015</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2244</th>\n",
       "      <td>20185</td>\n",
       "      <td>2205</td>\n",
       "      <td>2963004507</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['correct', 'assumption', 'algorithm', 'produc...</td>\n",
       "      <td>['Justin Eldridge', 'Mikhail Belkin', 'Yusu Wa...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Clustering coefficient', 'Cluster analysis',...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>Graphons, mergeons, and so on!</td>\n",
       "      <td>2016</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2245</th>\n",
       "      <td>20188</td>\n",
       "      <td>2206</td>\n",
       "      <td>2004669647</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['hyper', 'recordings', 'noise', 'means', 'tak...</td>\n",
       "      <td>['Justin Eldridge', 'Alison E Lane', 'Mikhail ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Electroencephalography', 'Autism spectrum di...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>journal of neurodevelopmental disorders</td>\n",
       "      <td>26</td>\n",
       "      <td>Robust features for the automatic identificati...</td>\n",
       "      <td>2014</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2246</th>\n",
       "      <td>20192</td>\n",
       "      <td>2207</td>\n",
       "      <td>1895840270</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['characterize', 'extensions', 'pi', 'singular...</td>\n",
       "      <td>['Aaron McMillan Fraenkel']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Isolated singularity', 'Koszul complex', 'Id...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>arxiv symplectic geometry</td>\n",
       "      <td>1</td>\n",
       "      <td>Extensions of Poisson Structures on Singular H...</td>\n",
       "      <td>2013</td>\n",
       "      <td>Aaron McMillan Fraenkel</td>\n",
       "      <td>Aaron McMillan Fraenkel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2247 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  \\\n",
       "0         3           0             0             0.0   \n",
       "1        15           1             1             1.0   \n",
       "2        24           2             2             2.0   \n",
       "3        35           3             3             3.0   \n",
       "4        46           4             4             4.0   \n",
       "...     ...         ...           ...             ...   \n",
       "2242  20179        2203    2963037546             NaN   \n",
       "2243  20182        2204    2963918728             NaN   \n",
       "2244  20185        2205    2963004507             NaN   \n",
       "2245  20188        2206    2004669647             NaN   \n",
       "2246  20192        2207    1895840270             NaN   \n",
       "\n",
       "                                               abstract  \\\n",
       "0                                                   NaN   \n",
       "1     Understanding of neuronal circuitry at cellula...   \n",
       "2     We study Vietoris–Rips complexes of metric wed...   \n",
       "3     Neuroscientific data analysis has traditionall...   \n",
       "4     Neuroscientific data analysis has traditionall...   \n",
       "...                                                 ...   \n",
       "2242  ['weyl', 'algorithm', 'typical', 'davis', 'unp...   \n",
       "2243  ['correct', 'imply', 'single', 'nesting', 'two...   \n",
       "2244  ['correct', 'assumption', 'algorithm', 'produc...   \n",
       "2245  ['hyper', 'recordings', 'noise', 'means', 'tak...   \n",
       "2246  ['characterize', 'extensions', 'pi', 'singular...   \n",
       "\n",
       "                                                authors  authors_count  \\\n",
       "0     [{'affiliations': [], 'corresponding': '', 'cu...            NaN   \n",
       "1     [{'affiliations': [{'city': 'Cold Spring Harbo...            NaN   \n",
       "2     [{'affiliations': [{'name': 'MOSEK ApS, Copenh...            NaN   \n",
       "3     [{'affiliations': [{'city': 'Columbus', 'city_...            NaN   \n",
       "4     [{'affiliations': [], 'corresponding': '', 'cu...            NaN   \n",
       "...                                                 ...            ...   \n",
       "2242  ['Justin Eldridge', 'Mikhail Belkin', 'Yusu Wa...            3.0   \n",
       "2243  ['Justin Eldridge', 'Mikhail Belkin', 'Yusu Wa...            3.0   \n",
       "2244  ['Justin Eldridge', 'Mikhail Belkin', 'Yusu Wa...            3.0   \n",
       "2245  ['Justin Eldridge', 'Alison E Lane', 'Mikhail ...            4.0   \n",
       "2246                        ['Aaron McMillan Fraenkel']            1.0   \n",
       "\n",
       "                                           category_for  \\\n",
       "0                                                   NaN   \n",
       "1     [{'id': '2208', 'name': '08 Information and Co...   \n",
       "2     [{'id': '2201', 'name': '01 Mathematical Scien...   \n",
       "3     [{'id': '3120', 'name': '1109 Neurosciences'},...   \n",
       "4     [{'id': '3120', 'name': '1109 Neurosciences'},...   \n",
       "...                                                 ...   \n",
       "2242                                                NaN   \n",
       "2243                                                NaN   \n",
       "2244                                                NaN   \n",
       "2245                                                NaN   \n",
       "2246                                                NaN   \n",
       "\n",
       "                                               concepts        date  \\\n",
       "0                            ['space', 'metric spaces']  2021-01-01   \n",
       "1     ['hybrid architecture', 'semantic segmentation...  2020-09-28   \n",
       "2     ['Vietoris–Rips complexes', 'wedge sum', 'metr...  2020-05-20   \n",
       "3     ['collection of neurons', 'hand-tuned paramete...  2020-03-22   \n",
       "4     ['collection of neurons', 'hand-tuned paramete...  2020-03-20   \n",
       "...                                                 ...         ...   \n",
       "2242  ['Perturbation theory', 'Eigenvalues and eigen...         NaN   \n",
       "2243  ['Hierarchical clustering', 'Cluster analysis'...         NaN   \n",
       "2244  ['Clustering coefficient', 'Cluster analysis',...         NaN   \n",
       "2245  ['Electroencephalography', 'Autism spectrum di...         NaN   \n",
       "2246  ['Isolated singularity', 'Koszul complex', 'Id...         NaN   \n",
       "\n",
       "                  id    journal.id  \\\n",
       "0     pub.1140323831  jour.1292375   \n",
       "1     pub.1131237716  jour.1336255   \n",
       "2     pub.1127764757  jour.1290431   \n",
       "3     pub.1125823510  jour.1293558   \n",
       "4     pub.1126276621  jour.1371339   \n",
       "...              ...           ...   \n",
       "2242             NaN           NaN   \n",
       "2243             NaN           NaN   \n",
       "2244             NaN           NaN   \n",
       "2245             NaN           NaN   \n",
       "2246             NaN           NaN   \n",
       "\n",
       "                                      journal.title  times_cited  \\\n",
       "0      SIAM Journal on Applied Algebra and Geometry            0   \n",
       "1                       Nature Machine Intelligence            3   \n",
       "2     Journal of Applied and Computational Topology            5   \n",
       "3                                           bioRxiv            0   \n",
       "4                                             arXiv            0   \n",
       "...                                             ...          ...   \n",
       "2242                                            NaN           50   \n",
       "2243                                            NaN           26   \n",
       "2244                                            NaN           13   \n",
       "2245        journal of neurodevelopmental disorders           26   \n",
       "2246                      arxiv symplectic geometry            1   \n",
       "\n",
       "                                                  title  year  \\\n",
       "0     Elder-Rule-Staircodes for Augmented Metric Spaces  2021   \n",
       "1     Semantic segmentation of microscopic neuroanat...  2020   \n",
       "2     On homotopy types of Vietoris–Rips complexes o...  2020   \n",
       "3     Detection and skeletonization of single neuron...  2020   \n",
       "4     Detection and skeletonization of single neuron...  2020   \n",
       "...                                                 ...   ...   \n",
       "2242  Unperturbed: spectral analysis beyond Davis-Kahan  2018   \n",
       "2243  Beyond Hartigan Consistency: Merge Distortion ...  2015   \n",
       "2244                     Graphons, mergeons, and so on!  2016   \n",
       "2245  Robust features for the automatic identificati...  2014   \n",
       "2246  Extensions of Poisson Structures on Singular H...  2013   \n",
       "\n",
       "                        names                      ids  \n",
       "0                   Yusu Wang        ur.01357524473.18  \n",
       "1                   Yusu Wang        ur.01357524473.18  \n",
       "2                   Yusu Wang        ur.01357524473.18  \n",
       "3                   Yusu Wang        ur.01357524473.18  \n",
       "4                   Yusu Wang        ur.01357524473.18  \n",
       "...                       ...                      ...  \n",
       "2242          Justin Eldridge          Justin Eldridge  \n",
       "2243          Justin Eldridge          Justin Eldridge  \n",
       "2244          Justin Eldridge          Justin Eldridge  \n",
       "2245          Justin Eldridge          Justin Eldridge  \n",
       "2246  Aaron McMillan Fraenkel  Aaron McMillan Fraenkel  \n",
       "\n",
       "[2247 rows x 18 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ur.01357524473.18', 'ur.013664734211.64', 'ur.013314650073.73',\n",
       "       'ur.015666002121.28', 'ur.01101412763.08', 'ur.014341701615.68',\n",
       "       'ur.016311745377.96', 'ur.014432230767.58', 'ur.0700320656.13',\n",
       "       'ur.01305066232.03', 'ur.012616664775.11', 'ur.01154535272.73',\n",
       "       'ur.010412405075.65', 'ur.015225361167.83', 'ur.0762250074.37',\n",
       "       'ur.01135040542.06', 'ur.0645152177.66', 'ur.011716527505.90',\n",
       "       'ur.016136746753.20', 'ur.011713373525.80', 'ur.01362460552.72',\n",
       "       'ur.07600460715.65', 'ur.010165372632.44', 'ur.010063005435.08',\n",
       "       'ur.07640676415.86', 'ur.0743424166.56', 'ur.01227437167.12',\n",
       "       'ur.0626741406.70', 'ur.010352472161.19', 'ur.01344500774.59',\n",
       "       'ur.01112612304.93', 'ur.014372657504.90', 'ur.010103102507.96',\n",
       "       'ur.016005750232.03', 'ur.0640005124.03', 'ur.01075673334.04',\n",
       "       'ur.0676635572.47', 'ur.013410772567.24', 'ur.0624011064.54',\n",
       "       'ur.01040634313.74', 'ur.01164165070.18', 'ur.01211157341.57',\n",
       "       'ur.01365426624.74', 'ur.010341750231.53', 'ur.014531026363.20',\n",
       "       'ur.01013646164.40', 'ur.01315753464.85', 'ur.013751023655.35',\n",
       "       'ur.01307465563.00', 'ur.01353530614.14', 'ur.01143047214.87',\n",
       "       'ur.0645001654.62', 'Justin Eldridge', 'Aaron McMillan Fraenkel'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['ids'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['abstract'] = df3['abstract'].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "redundant = ['abstract', 'purpose', 'paper', 'goal', 'usepackage', 'cod']\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "def preprocess_abstract(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in redundant:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return \" \".join(result)\n",
    "\n",
    "def standardize_abstract(abstract):\n",
    "    abstract = abstract.replace('\\n', ' ')\n",
    "    abstract = abstract.replace('  ', ' ')\n",
    "    abstract = abstract.replace('-', ' ')\n",
    "    abstract = abstract.replace('.', '')\n",
    "    abstract = abstract.replace(':', '')\n",
    "    abstract = abstract.replace(';', '')\n",
    "    abstract = abstract.replace(',', '')\n",
    "    abstract = abstract.replace('\"', '')\n",
    "    abstract = abstract.lower()\n",
    "    return abstract\n",
    "\n",
    "def standardize_title(title):\n",
    "    title = title.replace('\\n', ' ')\n",
    "    title = title.replace('  ', ' ')\n",
    "    title = title.replace('-', ' ')\n",
    "    title = title.replace('.', '')\n",
    "    title = title.replace(':', '')\n",
    "    title = title.replace(';', '')\n",
    "    title = title.replace(',', '')\n",
    "    title = title.replace('\"', '')\n",
    "    title = title.lower()\n",
    "df3['year'] = df3['year'].astype(int)\n",
    "df3['abstract'] = [standardize_abstract(text) for text in df3['abstract']]\n",
    "df3['title_standardized'] = [standardize_title(text) for text in df3['title']]\n",
    "df3['abstract_processed'] = df3['abstract'].apply(preprocess_abstract)\n",
    "df3.drop_duplicates(inplace=True, subset=['abstract'])\n",
    "df3.drop_duplicates(inplace=True, subset=['title_standardized'])\n",
    "df3.drop_duplicates(inplace=True, subset=['abstract_processed'])\n",
    "df3.dropna(axis=0, how='any')\n",
    "df3.reset_index(inplace=True)\n",
    "df3.drop(axis=1, labels=['index'], inplace=True)\n",
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant = ['abstract', 'purpose', 'paper', 'goal', 'usepackage', 'cod']\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "def preprocess_abstract(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in redundant:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return \" \".join(result)\n",
    "\n",
    "\n",
    "df3['abstract_processed'] = df3['abstract'].apply(preprocess_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>authors_count</th>\n",
       "      <th>category_for</th>\n",
       "      <th>concepts</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>journal.id</th>\n",
       "      <th>journal.title</th>\n",
       "      <th>times_cited</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>names</th>\n",
       "      <th>ids</th>\n",
       "      <th>abstract_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>[{'affiliations': [], 'corresponding': '', 'cu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['space', 'metric spaces']</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>pub.1140323831</td>\n",
       "      <td>jour.1292375</td>\n",
       "      <td>SIAM Journal on Applied Algebra and Geometry</td>\n",
       "      <td>0</td>\n",
       "      <td>Elder-Rule-Staircodes for Augmented Metric Spaces</td>\n",
       "      <td>2021</td>\n",
       "      <td>Yusu Wang</td>\n",
       "      <td>ur.01357524473.18</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Understanding of neuronal circuitry at cellula...</td>\n",
       "      <td>[{'affiliations': [{'city': 'Cold Spring Harbo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'id': '2208', 'name': '08 Information and Co...</td>\n",
       "      <td>['hybrid architecture', 'semantic segmentation...</td>\n",
       "      <td>2020-09-28</td>\n",
       "      <td>pub.1131237716</td>\n",
       "      <td>jour.1336255</td>\n",
       "      <td>Nature Machine Intelligence</td>\n",
       "      <td>3</td>\n",
       "      <td>Semantic segmentation of microscopic neuroanat...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Yusu Wang</td>\n",
       "      <td>ur.01357524473.18</td>\n",
       "      <td>understand neuronal circuitry cellular resolut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>We study Vietoris–Rips complexes of metric wed...</td>\n",
       "      <td>[{'affiliations': [{'name': 'MOSEK ApS, Copenh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'id': '2201', 'name': '01 Mathematical Scien...</td>\n",
       "      <td>['Vietoris–Rips complexes', 'wedge sum', 'metr...</td>\n",
       "      <td>2020-05-20</td>\n",
       "      <td>pub.1127764757</td>\n",
       "      <td>jour.1290431</td>\n",
       "      <td>Journal of Applied and Computational Topology</td>\n",
       "      <td>5</td>\n",
       "      <td>On homotopy types of Vietoris–Rips complexes o...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Yusu Wang</td>\n",
       "      <td>ur.01357524473.18</td>\n",
       "      <td>study vietoris rip complexes metric wedge sum ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neuroscientific data analysis has traditionall...</td>\n",
       "      <td>[{'affiliations': [{'city': 'Columbus', 'city_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'id': '3120', 'name': '1109 Neurosciences'},...</td>\n",
       "      <td>['collection of neurons', 'hand-tuned paramete...</td>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>pub.1125823510</td>\n",
       "      <td>jour.1293558</td>\n",
       "      <td>bioRxiv</td>\n",
       "      <td>0</td>\n",
       "      <td>Detection and skeletonization of single neuron...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Yusu Wang</td>\n",
       "      <td>ur.01357524473.18</td>\n",
       "      <td>neuroscientific data analysis traditionally re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Neuroscientific data analysis has traditionall...</td>\n",
       "      <td>[{'affiliations': [], 'corresponding': '', 'cu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'id': '3120', 'name': '1109 Neurosciences'},...</td>\n",
       "      <td>['collection of neurons', 'hand-tuned paramete...</td>\n",
       "      <td>2020-03-20</td>\n",
       "      <td>pub.1126276621</td>\n",
       "      <td>jour.1371339</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>0</td>\n",
       "      <td>Detection and skeletonization of single neuron...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Yusu Wang</td>\n",
       "      <td>ur.01357524473.18</td>\n",
       "      <td>neuroscientific data analysis traditionally re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2242</th>\n",
       "      <td>20179</td>\n",
       "      <td>2203</td>\n",
       "      <td>2963037546</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['weyl', 'algorithm', 'typical', 'davis', 'unp...</td>\n",
       "      <td>['Justin Eldridge', 'Mikhail Belkin', 'Yusu Wa...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Perturbation theory', 'Eigenvalues and eigen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>Unperturbed: spectral analysis beyond Davis-Kahan</td>\n",
       "      <td>2018</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "      <td>weyl algorithm typical davis unperturbed tool ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2243</th>\n",
       "      <td>20182</td>\n",
       "      <td>2204</td>\n",
       "      <td>2963918728</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['correct', 'imply', 'single', 'nesting', 'two...</td>\n",
       "      <td>['Justin Eldridge', 'Mikhail Belkin', 'Yusu Wa...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Hierarchical clustering', 'Cluster analysis'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>Beyond Hartigan Consistency: Merge Distortion ...</td>\n",
       "      <td>2015</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "      <td>correct imply single nest improper view popula...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2244</th>\n",
       "      <td>20185</td>\n",
       "      <td>2205</td>\n",
       "      <td>2963004507</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['correct', 'assumption', 'algorithm', 'produc...</td>\n",
       "      <td>['Justin Eldridge', 'Mikhail Belkin', 'Yusu Wa...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Clustering coefficient', 'Cluster analysis',...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>Graphons, mergeons, and so on!</td>\n",
       "      <td>2016</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "      <td>correct assumption algorithm produce graphons ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2245</th>\n",
       "      <td>20188</td>\n",
       "      <td>2206</td>\n",
       "      <td>2004669647</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['hyper', 'recordings', 'noise', 'means', 'tak...</td>\n",
       "      <td>['Justin Eldridge', 'Alison E Lane', 'Mikhail ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Electroencephalography', 'Autism spectrum di...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>journal of neurodevelopmental disorders</td>\n",
       "      <td>26</td>\n",
       "      <td>Robust features for the automatic identificati...</td>\n",
       "      <td>2014</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "      <td>hyper record noise mean take robust efficacy a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2246</th>\n",
       "      <td>20192</td>\n",
       "      <td>2207</td>\n",
       "      <td>1895840270</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['characterize', 'extensions', 'pi', 'singular...</td>\n",
       "      <td>['Aaron McMillan Fraenkel']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Isolated singularity', 'Koszul complex', 'Id...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>arxiv symplectic geometry</td>\n",
       "      <td>1</td>\n",
       "      <td>Extensions of Poisson Structures on Singular H...</td>\n",
       "      <td>2013</td>\n",
       "      <td>Aaron McMillan Fraenkel</td>\n",
       "      <td>Aaron McMillan Fraenkel</td>\n",
       "      <td>characterize extensions singular koszul singul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2247 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  \\\n",
       "0         3           0             0             0.0   \n",
       "1        15           1             1             1.0   \n",
       "2        24           2             2             2.0   \n",
       "3        35           3             3             3.0   \n",
       "4        46           4             4             4.0   \n",
       "...     ...         ...           ...             ...   \n",
       "2242  20179        2203    2963037546             NaN   \n",
       "2243  20182        2204    2963918728             NaN   \n",
       "2244  20185        2205    2963004507             NaN   \n",
       "2245  20188        2206    2004669647             NaN   \n",
       "2246  20192        2207    1895840270             NaN   \n",
       "\n",
       "                                               abstract  \\\n",
       "0                                                         \n",
       "1     Understanding of neuronal circuitry at cellula...   \n",
       "2     We study Vietoris–Rips complexes of metric wed...   \n",
       "3     Neuroscientific data analysis has traditionall...   \n",
       "4     Neuroscientific data analysis has traditionall...   \n",
       "...                                                 ...   \n",
       "2242  ['weyl', 'algorithm', 'typical', 'davis', 'unp...   \n",
       "2243  ['correct', 'imply', 'single', 'nesting', 'two...   \n",
       "2244  ['correct', 'assumption', 'algorithm', 'produc...   \n",
       "2245  ['hyper', 'recordings', 'noise', 'means', 'tak...   \n",
       "2246  ['characterize', 'extensions', 'pi', 'singular...   \n",
       "\n",
       "                                                authors  authors_count  \\\n",
       "0     [{'affiliations': [], 'corresponding': '', 'cu...            NaN   \n",
       "1     [{'affiliations': [{'city': 'Cold Spring Harbo...            NaN   \n",
       "2     [{'affiliations': [{'name': 'MOSEK ApS, Copenh...            NaN   \n",
       "3     [{'affiliations': [{'city': 'Columbus', 'city_...            NaN   \n",
       "4     [{'affiliations': [], 'corresponding': '', 'cu...            NaN   \n",
       "...                                                 ...            ...   \n",
       "2242  ['Justin Eldridge', 'Mikhail Belkin', 'Yusu Wa...            3.0   \n",
       "2243  ['Justin Eldridge', 'Mikhail Belkin', 'Yusu Wa...            3.0   \n",
       "2244  ['Justin Eldridge', 'Mikhail Belkin', 'Yusu Wa...            3.0   \n",
       "2245  ['Justin Eldridge', 'Alison E Lane', 'Mikhail ...            4.0   \n",
       "2246                        ['Aaron McMillan Fraenkel']            1.0   \n",
       "\n",
       "                                           category_for  \\\n",
       "0                                                   NaN   \n",
       "1     [{'id': '2208', 'name': '08 Information and Co...   \n",
       "2     [{'id': '2201', 'name': '01 Mathematical Scien...   \n",
       "3     [{'id': '3120', 'name': '1109 Neurosciences'},...   \n",
       "4     [{'id': '3120', 'name': '1109 Neurosciences'},...   \n",
       "...                                                 ...   \n",
       "2242                                                NaN   \n",
       "2243                                                NaN   \n",
       "2244                                                NaN   \n",
       "2245                                                NaN   \n",
       "2246                                                NaN   \n",
       "\n",
       "                                               concepts        date  \\\n",
       "0                            ['space', 'metric spaces']  2021-01-01   \n",
       "1     ['hybrid architecture', 'semantic segmentation...  2020-09-28   \n",
       "2     ['Vietoris–Rips complexes', 'wedge sum', 'metr...  2020-05-20   \n",
       "3     ['collection of neurons', 'hand-tuned paramete...  2020-03-22   \n",
       "4     ['collection of neurons', 'hand-tuned paramete...  2020-03-20   \n",
       "...                                                 ...         ...   \n",
       "2242  ['Perturbation theory', 'Eigenvalues and eigen...         NaN   \n",
       "2243  ['Hierarchical clustering', 'Cluster analysis'...         NaN   \n",
       "2244  ['Clustering coefficient', 'Cluster analysis',...         NaN   \n",
       "2245  ['Electroencephalography', 'Autism spectrum di...         NaN   \n",
       "2246  ['Isolated singularity', 'Koszul complex', 'Id...         NaN   \n",
       "\n",
       "                  id    journal.id  \\\n",
       "0     pub.1140323831  jour.1292375   \n",
       "1     pub.1131237716  jour.1336255   \n",
       "2     pub.1127764757  jour.1290431   \n",
       "3     pub.1125823510  jour.1293558   \n",
       "4     pub.1126276621  jour.1371339   \n",
       "...              ...           ...   \n",
       "2242             NaN           NaN   \n",
       "2243             NaN           NaN   \n",
       "2244             NaN           NaN   \n",
       "2245             NaN           NaN   \n",
       "2246             NaN           NaN   \n",
       "\n",
       "                                      journal.title  times_cited  \\\n",
       "0      SIAM Journal on Applied Algebra and Geometry            0   \n",
       "1                       Nature Machine Intelligence            3   \n",
       "2     Journal of Applied and Computational Topology            5   \n",
       "3                                           bioRxiv            0   \n",
       "4                                             arXiv            0   \n",
       "...                                             ...          ...   \n",
       "2242                                            NaN           50   \n",
       "2243                                            NaN           26   \n",
       "2244                                            NaN           13   \n",
       "2245        journal of neurodevelopmental disorders           26   \n",
       "2246                      arxiv symplectic geometry            1   \n",
       "\n",
       "                                                  title  year  \\\n",
       "0     Elder-Rule-Staircodes for Augmented Metric Spaces  2021   \n",
       "1     Semantic segmentation of microscopic neuroanat...  2020   \n",
       "2     On homotopy types of Vietoris–Rips complexes o...  2020   \n",
       "3     Detection and skeletonization of single neuron...  2020   \n",
       "4     Detection and skeletonization of single neuron...  2020   \n",
       "...                                                 ...   ...   \n",
       "2242  Unperturbed: spectral analysis beyond Davis-Kahan  2018   \n",
       "2243  Beyond Hartigan Consistency: Merge Distortion ...  2015   \n",
       "2244                     Graphons, mergeons, and so on!  2016   \n",
       "2245  Robust features for the automatic identificati...  2014   \n",
       "2246  Extensions of Poisson Structures on Singular H...  2013   \n",
       "\n",
       "                        names                      ids  \\\n",
       "0                   Yusu Wang        ur.01357524473.18   \n",
       "1                   Yusu Wang        ur.01357524473.18   \n",
       "2                   Yusu Wang        ur.01357524473.18   \n",
       "3                   Yusu Wang        ur.01357524473.18   \n",
       "4                   Yusu Wang        ur.01357524473.18   \n",
       "...                       ...                      ...   \n",
       "2242          Justin Eldridge          Justin Eldridge   \n",
       "2243          Justin Eldridge          Justin Eldridge   \n",
       "2244          Justin Eldridge          Justin Eldridge   \n",
       "2245          Justin Eldridge          Justin Eldridge   \n",
       "2246  Aaron McMillan Fraenkel  Aaron McMillan Fraenkel   \n",
       "\n",
       "                                     abstract_processed  \n",
       "0                                                        \n",
       "1     understand neuronal circuitry cellular resolut...  \n",
       "2     study vietoris rip complexes metric wedge sum ...  \n",
       "3     neuroscientific data analysis traditionally re...  \n",
       "4     neuroscientific data analysis traditionally re...  \n",
       "...                                                 ...  \n",
       "2242  weyl algorithm typical davis unperturbed tool ...  \n",
       "2243  correct imply single nest improper view popula...  \n",
       "2244  correct assumption algorithm produce graphons ...  \n",
       "2245  hyper record noise mean take robust efficacy a...  \n",
       "2246  characterize extensions singular koszul singul...  \n",
       "\n",
       "[2247 rows x 19 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3[df3['year'] >= 2015]\n",
    "counts = CountVectorizer().fit_transform(df3['abstract_processed'])\n",
    "authors = {}\n",
    "for author in df3.names.unique():\n",
    "    authors[author] = {\n",
    "        2015 : list(),\n",
    "        2016 : list(),\n",
    "        2017 : list(),\n",
    "        2018 : list(),\n",
    "        2019 : list(),\n",
    "        2020 : list(),\n",
    "        2021 : list()\n",
    "    }\n",
    "for i, row in df3.iterrows():\n",
    "    authors[row['names']][row['year']].append(row['abstract_processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_docs = []\n",
    "#for author, author_dict in authors.items():\n",
    "#    for year, documents in author_dict.items():\n",
    "#        all_docs.append(\" \".join(documents))\n",
    "#len(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "381"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs = []\n",
    "missing_author_years = {author : list() for author in df3.names.unique()}\n",
    "for author, author_dict in authors.items():\n",
    "    for year, documents in author_dict.items():\n",
    "        if len(documents) == 0:\n",
    "            missing_author_years[author].append(year)\n",
    "            continue\n",
    "        all_docs.append(\" \".join(documents))\n",
    "len(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initate LDA model\n",
    "countVec = CountVectorizer()\n",
    "counts = countVec.fit_transform(all_docs)\n",
    "names = countVec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n"
     ]
    }
   ],
   "source": [
    "modeller = LatentDirichletAllocation(n_components=10, n_jobs=-1, random_state=123)\n",
    "result = modeller.fit_transform(counts)\n",
    "modeller2 = LatentDirichletAllocation(n_components=20, n_jobs=-1, random_state=123)\n",
    "result2 = modeller2.fit_transform(counts)\n",
    "modeller3 = LatentDirichletAllocation(n_components=30, n_jobs=-1, random_state=123)\n",
    "result3 = modeller3.fit_transform(counts)\n",
    "modeller4 = LatentDirichletAllocation(n_components=40, n_jobs=-1, random_state=123)\n",
    "result4 = modeller4.fit_transform(counts)\n",
    "modeller5 = LatentDirichletAllocation(n_components=50, n_jobs=-1, random_state=123)\n",
    "result5 = modeller5.fit_transform(counts)\n",
    "\n",
    "models = {'10':modeller,'20':modeller2,'30':modeller3,'40':modeller4,'50':modeller5}\n",
    "results = {'10':result,'20':result2,'30':result3,'40':result4,'50':result5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "topicnames = {\n",
    "    num_topics : [\"Topic\" + str(i) for i in range(num_topics)] for num_topics in range(10, 60, 10)\n",
    "}\n",
    "\n",
    "# index names\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(all_docs))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = {\n",
    "    num_topics : pd.DataFrame(results[f'{num_topics}'], columns=topicnames[num_topics], index=docnames) for num_topics in range(10, 60, 10)\n",
    "}\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = {\n",
    "    num_topics : np.argmax(df_document_topic[num_topics].values, axis=1) for num_topics in range(10, 60, 10)\n",
    "}\n",
    "\n",
    "for num_topics, df in df_document_topic.items():\n",
    "    df['dominant_topic'] = dominant_topic[num_topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_list = []\n",
    "year_list = []\n",
    "for author in authors.keys():\n",
    "    for i in range(7):\n",
    "        if (2015 + i) not in missing_author_years[author]:\n",
    "            author_list.append(author)\n",
    "            year_list.append(2015 + i)\n",
    "\n",
    "for df in df_document_topic.values():\n",
    "    df['author'] = author_list\n",
    "    df['year'] = year_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic0</th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "      <th>Topic3</th>\n",
       "      <th>Topic4</th>\n",
       "      <th>Topic5</th>\n",
       "      <th>Topic6</th>\n",
       "      <th>Topic7</th>\n",
       "      <th>Topic8</th>\n",
       "      <th>Topic9</th>\n",
       "      <th>...</th>\n",
       "      <th>Topic43</th>\n",
       "      <th>Topic44</th>\n",
       "      <th>Topic45</th>\n",
       "      <th>Topic46</th>\n",
       "      <th>Topic47</th>\n",
       "      <th>Topic48</th>\n",
       "      <th>Topic49</th>\n",
       "      <th>dominant_topic</th>\n",
       "      <th>author</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc0</th>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>29</td>\n",
       "      <td>Yusu Wang</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>29</td>\n",
       "      <td>Yusu Wang</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2</th>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>31</td>\n",
       "      <td>Yusu Wang</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc3</th>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>29</td>\n",
       "      <td>Yusu Wang</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc4</th>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>29</td>\n",
       "      <td>Yusu Wang</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc376</th>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0</td>\n",
       "      <td>G Sugihara</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc377</th>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0</td>\n",
       "      <td>GEORGE SUGIHARA</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc378</th>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.131214</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>39</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc379</th>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.088831</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.215321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.238058</td>\n",
       "      <td>35</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc380</th>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.142771</td>\n",
       "      <td>0.228750</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.146018</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>21</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>381 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Topic0    Topic1    Topic2    Topic3    Topic4    Topic5    Topic6  \\\n",
       "Doc0    0.000075  0.000075  0.000075  0.000075  0.000075  0.000075  0.000075   \n",
       "Doc1    0.000019  0.000019  0.000019  0.000019  0.000019  0.000019  0.000019   \n",
       "Doc2    0.000024  0.000024  0.000024  0.000024  0.000024  0.000024  0.000024   \n",
       "Doc3    0.000026  0.000026  0.000026  0.000026  0.000026  0.000026  0.000026   \n",
       "Doc4    0.000031  0.000031  0.000031  0.000031  0.000031  0.000031  0.000031   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "Doc376  0.020000  0.020000  0.020000  0.020000  0.020000  0.020000  0.020000   \n",
       "Doc377  0.020000  0.020000  0.020000  0.020000  0.020000  0.020000  0.020000   \n",
       "Doc378  0.000270  0.000270  0.000270  0.000270  0.000270  0.000270  0.000270   \n",
       "Doc379  0.000476  0.000476  0.000476  0.088831  0.000476  0.000476  0.000476   \n",
       "Doc380  0.000417  0.000417  0.142771  0.228750  0.000417  0.000417  0.146018   \n",
       "\n",
       "          Topic7    Topic8    Topic9  ...   Topic43   Topic44   Topic45  \\\n",
       "Doc0    0.000075  0.000075  0.000075  ...  0.000075  0.000075  0.000075   \n",
       "Doc1    0.000019  0.000019  0.000019  ...  0.000019  0.000019  0.000019   \n",
       "Doc2    0.000024  0.000024  0.000024  ...  0.000024  0.000024  0.000024   \n",
       "Doc3    0.000026  0.000026  0.000026  ...  0.000026  0.000026  0.000026   \n",
       "Doc4    0.000031  0.000031  0.000031  ...  0.000031  0.000031  0.000031   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "Doc376  0.020000  0.020000  0.020000  ...  0.020000  0.020000  0.020000   \n",
       "Doc377  0.020000  0.020000  0.020000  ...  0.020000  0.020000  0.020000   \n",
       "Doc378  0.000270  0.131214  0.000270  ...  0.000270  0.000270  0.000270   \n",
       "Doc379  0.000476  0.000476  0.215321  ...  0.000476  0.000476  0.000476   \n",
       "Doc380  0.000417  0.000417  0.000417  ...  0.000417  0.000417  0.000417   \n",
       "\n",
       "         Topic46   Topic47   Topic48   Topic49  dominant_topic  \\\n",
       "Doc0    0.000075  0.000075  0.000075  0.000075              29   \n",
       "Doc1    0.000019  0.000019  0.000019  0.000019              29   \n",
       "Doc2    0.000024  0.000024  0.000024  0.000024              31   \n",
       "Doc3    0.000026  0.000026  0.000026  0.000026              29   \n",
       "Doc4    0.000031  0.000031  0.000031  0.000031              29   \n",
       "...          ...       ...       ...       ...             ...   \n",
       "Doc376  0.020000  0.020000  0.020000  0.020000               0   \n",
       "Doc377  0.020000  0.020000  0.020000  0.020000               0   \n",
       "Doc378  0.000270  0.000270  0.000270  0.000270              39   \n",
       "Doc379  0.000476  0.000476  0.000476  0.238058              35   \n",
       "Doc380  0.000417  0.000417  0.000417  0.000417              21   \n",
       "\n",
       "                 author  year  \n",
       "Doc0          Yusu Wang  2016  \n",
       "Doc1          Yusu Wang  2017  \n",
       "Doc2          Yusu Wang  2018  \n",
       "Doc3          Yusu Wang  2019  \n",
       "Doc4          Yusu Wang  2020  \n",
       "...                 ...   ...  \n",
       "Doc376       G Sugihara  2017  \n",
       "Doc377  GEORGE SUGIHARA  2016  \n",
       "Doc378  Justin Eldridge  2015  \n",
       "Doc379  Justin Eldridge  2016  \n",
       "Doc380  Justin Eldridge  2018  \n",
       "\n",
       "[381 rows x 53 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged = {\n",
    "    num_topics : df_document_topic[num_topics].groupby('author').mean().drop(['dominant_topic', 'year'], axis=1) for num_topics in df_document_topic.keys()\n",
    "}\n",
    "\n",
    "filtered = {\n",
    "    threshold : {num_topics : averaged[num_topics].mask(averaged[num_topics] < threshold, other=0) for num_topics in averaged.keys()} for threshold in [.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "for num_topics in range(10, 60, 10):\n",
    "    labels[num_topics] = filtered[.1][num_topics].index.to_list()\n",
    "    labels[num_topics].extend(filtered[.1][num_topics].columns.to_list())\n",
    "\n",
    "\n",
    "sources = {threshold : {} for threshold in [.1]}\n",
    "targets = {threshold : {} for threshold in [.1]}\n",
    "values = {threshold : {} for threshold in [.1]}\n",
    "\n",
    "for threshold in [.1]:\n",
    "    for num_topics in range(10, 60, 10):\n",
    "        curr_sources = []\n",
    "        curr_targets = []\n",
    "        curr_values = []\n",
    "        index_counter = 0\n",
    "        for index, row in filtered[threshold][num_topics].iterrows():\n",
    "            for i, value in enumerate(row):\n",
    "                if value != 0:\n",
    "                    curr_sources.append(index_counter)\n",
    "                    curr_targets.append(108 + i)\n",
    "                    curr_values.append(value)\n",
    "            index_counter += 1\n",
    "        sources[threshold][num_topics] = curr_sources\n",
    "        targets[threshold][num_topics] = curr_targets\n",
    "        values[threshold][num_topics] = curr_values\n",
    "\n",
    "positions = {\n",
    "    num_topics : {label : i for i, label in enumerate(labels[num_topics])} for num_topics in averaged.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_ranks(array):\n",
    "    ranks = []\n",
    "    for value in array:\n",
    "        for i, percentage in enumerate(np.arange(.1, 1.1, .1)):\n",
    "            if value <= np.quantile(array, percentage):\n",
    "                ranks.append(i + 1)\n",
    "                break\n",
    "    return ranks\n",
    "\n",
    "final_values = {threshold : {} for threshold in [.1]}\n",
    "\n",
    "for threshold in [.1]:\n",
    "    for num_topics in range(10, 60, 10):\n",
    "        curr_values_array = np.array(values[threshold][num_topics])\n",
    "        final_values[threshold][num_topics] = split_into_ranks(curr_values_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics_list(model, feature_names, no_top_words):\n",
    "    topic_list = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_list.append(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "    return topic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_labels = {}\n",
    "for num_topics in range(10, 60, 10):\n",
    "    link_labels[num_topics] = labels[num_topics].copy()\n",
    "    link_labels[num_topics][50:] = display_topics_list(models[f'{num_topics}'], names, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n"
     ]
    }
   ],
   "source": [
    "counts = CountVectorizer().fit_transform(df3['abstract_processed'])\n",
    "transformed_list = []\n",
    "for model in models.values():\n",
    "    transformed_list.append(model.transform(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {threshold : {} for threshold in [.1]}\n",
    "for i, matrix in enumerate(transformed_list):\n",
    "    for threshold in [.1]:\n",
    "        df = pd.DataFrame(matrix)\n",
    "        df.mask(df < threshold, other=0, inplace=True)\n",
    "        df['author'] = df3['names']\n",
    "        df['year'] = df3['year']\n",
    "        df['citations'] = df3['times_cited'] + 1\n",
    "\n",
    "        # noralization of citations: Scaling to a range [0, 1]\n",
    "        df['citations_norm'] = df.groupby(by=['author', 'year'])['citations'].apply(lambda x: (x-x.min())/(x.max()-x.min()))#normalize_by_group(df=df, by=['author', 'year'])['citations']\n",
    "        df['abstract'] = df3['abstract']\n",
    "        df['title'] = df3['title']\n",
    "        df.fillna(1, inplace=True)\n",
    "        \n",
    "        #alpha weight parameter for weighting importance of citations vs topic relation\n",
    "        alpha = .75\n",
    "        for topic_num in range((i+1) * 10):\n",
    "            df[f'{topic_num}_relevance'] = alpha * df[topic_num] + (1-alpha) * df['citations_norm']\n",
    "        dataframes[threshold][(i+1) * 10] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([               0,                1,                2,                3,\n",
       "                      4,                5,                6,                7,\n",
       "                      8,                9,         'author',           'year',\n",
       "            'citations', 'citations_norm',       'abstract',          'title',\n",
       "          '0_relevance',    '1_relevance',    '2_relevance',    '3_relevance',\n",
       "          '4_relevance',    '5_relevance',    '6_relevance',    '7_relevance',\n",
       "          '8_relevance',    '9_relevance'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes[0.1][10].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_top_list(data_frame, num_topics, threshold):\n",
    "    top_5s = []\n",
    "    the_filter = filtered[threshold][num_topics]\n",
    "    for topic in range(num_topics):\n",
    "        relevant = the_filter[the_filter[f'Topic{topic}'] != 0].index.to_list()\n",
    "        to_append = data_frame[data_frame[f'{topic}_relevance'] > 0].reset_index()\n",
    "        to_append = to_append[to_append['author'].isin(relevant)].reset_index()\n",
    "        top_5s.append(to_append) \n",
    "    return top_5s\n",
    "\n",
    "tops = {\n",
    "    threshold : {num_topics : create_top_list(dataframes[threshold][num_topics], num_topics, threshold) for num_topics in range(10, 60, 10)} for threshold in [.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: \n",
      "The dash_core_components package is deprecated. Please replace\n",
      "`import dash_core_components as dcc` with `from dash import dcc`\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: \n",
      "The dash_html_components package is deprecated. Please replace\n",
      "`import dash_html_components as html` with `from dash import html`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import dash_bootstrap_components as dbc\n",
    "import plotly.graph_objects as go\n",
    "from dash.dependencies import Input, Output, State\n",
    "\n",
    "# sankey diagrams for diff numbers of topics\n",
    "\n",
    "heights = {\n",
    "  10 : 1000,\n",
    "  20 : 1500,\n",
    "  30 : 2000,\n",
    "  40 : 2500,\n",
    "  50 : 3000\n",
    "}\n",
    "\n",
    "figs = {threshold : {} for threshold in [.1]}\n",
    "for threshold in [.1]:\n",
    "    for num_topics in range(10, 60, 10):\n",
    "        fig = go.Figure(data=[go.Sankey(\n",
    "            node = dict(\n",
    "                pad = 15,\n",
    "                thickness = 20,\n",
    "                line = dict(color = 'black', width = 0.5),\n",
    "                label = labels[num_topics],\n",
    "                color = ['#666699' for i in range(len(labels[num_topics]))],\n",
    "                customdata = link_labels[num_topics],\n",
    "                hovertemplate='%{customdata} Total Flow: %{value}<extra></extra>'\n",
    "            ),\n",
    "            link = dict(\n",
    "                color = ['rgba(204, 204, 204, .5)' for i in range(len(sources[threshold][num_topics]))],\n",
    "                source = sources[threshold][num_topics],\n",
    "                target = targets[threshold][num_topics],\n",
    "                value = final_values[threshold][num_topics]\n",
    "            )\n",
    "        )])\n",
    "        fig.update_layout(title_text=\"Author Topic Connections\", font=dict(size = 10, color = 'white'), height=heights[num_topics], paper_bgcolor=\"black\", plot_bgcolor='black')\n",
    "        figs[threshold][num_topics] = fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2203    ['weyl', 'algorithm', 'typical', 'davis', 'unp...\n",
       "Name: abstract, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words = {\n",
    "    10 : display_topics_list(models['10'], names, 10),\n",
    "    20 : display_topics_list(models['20'], names, 10),\n",
    "    30 : display_topics_list(models['30'], names, 10),\n",
    "    40 : display_topics_list(models['40'], names, 10),\n",
    "    50 : display_topics_list(models['50'], names, 10)\n",
    "}\n",
    "\n",
    "combined = pd.read_csv('final_hdsi_faculty_updated.csv')\n",
    "combined[combined.title == 'Unperturbed: spectral analysis beyond Davis-Kahan'].abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = {}\n",
    "for i, word in enumerate(names):\n",
    "    locations[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:8050/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [04/Dec/2021 13:16:35] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Dec/2021 13:16:35] \"GET /_dash-component-suites/dash/dcc/async-graph.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [04/Dec/2021 13:16:35] \"GET /_dash-component-suites/dash/dcc/async-dropdown.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [04/Dec/2021 13:16:35] \"GET /_dash-component-suites/dash/dcc/async-plotlyjs.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [04/Dec/2021 13:16:35] \"GET /_dash-layout HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Dec/2021 13:16:35] \"GET /_dash-dependencies HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Dec/2021 13:16:35] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Dec/2021 13:16:37] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Dec/2021 13:16:37] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Dec/2021 13:16:40] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Dec/2021 13:16:40] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Dec/2021 13:16:40] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:150: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "127.0.0.1 - - [04/Dec/2021 13:17:08] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Dec/2021 13:17:09] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Dec/2021 13:49:35] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Dec/2021 13:49:35] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Dec/2021 13:49:39] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Dec/2021 13:49:39] \"POST /_dash-update-component HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "threshold = .1\n",
    "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.DARKLY])\n",
    "\n",
    "app.layout = html.Div([\n",
    "  dbc.Row([\n",
    "      dcc.Dropdown(\n",
    "        id='graph-dropdown',\n",
    "        placeholder='select number of LDA topics',\n",
    "        options=[{'label' : f'{i} Topic Model', 'value' : i} for i in range(10, 60, 10)],\n",
    "        style={\n",
    "          'color' : 'black',\n",
    "          'background-color' : '#666699',\n",
    "          'width' : '200%',\n",
    "          'align-items' : 'left',\n",
    "          'justify-content' : 'left',\n",
    "          'padding-left' : '15px'\n",
    "        },\n",
    "        value=10\n",
    "      )\n",
    "  ]),\n",
    "  dbc.Row([\n",
    "    dbc.Col(html.Div([\n",
    "      dcc.Graph(\n",
    "        id = 'graph',\n",
    "        figure = figs[.1][10]\n",
    "      )\n",
    "      ],\n",
    "      style={\n",
    "        'height' : '100vh',\n",
    "        'overflow-y' : 'scroll'\n",
    "      }\n",
    "    )\n",
    "    ),\n",
    "      dbc.Col(html.Div([dbc.Col([\n",
    "        dcc.Dropdown(\n",
    "          id='dropdown_menu',\n",
    "          placeholder='Select a topic',\n",
    "          options=[{'label' : f'Topic {topic}: {top_words[10][topic]}', 'value' : topic} for topic in range(10)],\n",
    "          style={\n",
    "            'color' : 'black',\n",
    "            'background-color' : 'white'\n",
    "          }\n",
    "        ),\n",
    "        dcc.Dropdown(\n",
    "          id='researcher-dropdown',\n",
    "          placeholder='Select Researchers',\n",
    "          options=[{'label' : f'{researcher}', 'value' : f'{researcher}'} for researcher in set(author_list)],\n",
    "          style={\n",
    "            'color' : 'black',\n",
    "            'background-color' : 'white'\n",
    "          }\n",
    "        )]),\n",
    "        dbc.Col(\n",
    "          dcc.Dropdown(\n",
    "            id='word-search',\n",
    "            placeholder='Search by word',\n",
    "            options=[{'label' : word, 'value' : word} for word in names],\n",
    "            style={\n",
    "              'color' : 'black',\n",
    "              'background-color' : 'white'\n",
    "            },\n",
    "            value=[],\n",
    "            multi=True\n",
    "          )\n",
    "        ),\n",
    "        html.Div(\n",
    "          id='paper_container', \n",
    "          children=[\n",
    "            html.P(\n",
    "              children=['Top 5 Papers'],\n",
    "              id='titles_and_authors', \n",
    "              draggable=False, \n",
    "              style={\n",
    "                'font-size' :'150%',\n",
    "                'font-family' : 'Verdana'\n",
    "              }\n",
    "            ),\n",
    "          ],\n",
    "        ),\n",
    "      ], \n",
    "        style={\n",
    "          'height' : '100vh',\n",
    "          'overflow-y' : 'scroll'\n",
    "        }\n",
    "      )\n",
    "      )\n",
    "    ]\n",
    "  )]\n",
    ")\n",
    "\n",
    "@app.callback(\n",
    "  Output('titles_and_authors', 'children'),\n",
    "  Output('researcher-dropdown', 'options'),\n",
    "  Input('dropdown_menu', 'value'),\n",
    "  Input('graph-dropdown', 'value'),\n",
    "  Input('researcher-dropdown', 'value'),\n",
    "  Input('word-search', 'value')\n",
    ")\n",
    "def update_p(topic, num_topics, author, words):\n",
    "  if len(words) != 0:\n",
    "    doc_vec = np.zeros((1, len(names)))\n",
    "    for word in words:\n",
    "      doc_vec[0][locations[word]] += 1\n",
    "    relations = np.round(models[f'{num_topics}'].transform(doc_vec), 5).tolist()[0]\n",
    "    pairs = [(i, relation) for i, relation in enumerate(relations)]\n",
    "    pairs.sort(reverse=True, key=lambda x: x[1])\n",
    "    to_return = [[html.Br(), f'Topic{pair[0]}: {pair[1]}', html.Br()] for pair in pairs]\n",
    "    return list(chain(*to_return)), [{'label' : f'{researcher}', 'value' : f'{researcher}'} for researcher in set(author_list)]\n",
    "\n",
    "  if topic == None and author == None:\n",
    "    return ['Make a selection'], [{'label' : f'{researcher}', 'value' : f'{researcher}'} for researcher in set(author_list)]\n",
    "\n",
    "  if topic != None and author == None:\n",
    "    df = tops[threshold][num_topics][topic]\n",
    "    df_authors = df.author.unique()\n",
    "    max_vals = df.groupby('author').max()[f'{topic}_relevance']\n",
    "\n",
    "    to_return = [[f'{name}:', html.Br(), \n",
    "      f'{df[df[f\"{topic}_relevance\"] == max_vals.loc[name]][\"title\"].to_list()[0]}',\n",
    "      html.Details([html.Summary('Abstract'),\n",
    "                    html.Div(combined[combined.title == f'{df[df[f\"{topic}_relevance\"] == max_vals.loc[name]][\"title\"].to_list()[0]}'].abstract)],\n",
    "                    style={\n",
    "                      'font-size' :'80%',\n",
    "                      'font-family' : 'Verdana'}),\n",
    "      html.Br()] for i, name in enumerate(max_vals.index)]\n",
    "    return list(chain(*to_return)), [{'label' : f'{researcher}', 'value' : f'{researcher}'} for researcher in tops[threshold][num_topics][topic].author.unique()]\n",
    "\n",
    "  if topic == None and author != None:\n",
    "    to_return = []\n",
    "    for topic_num in range(num_topics):\n",
    "      df = tops[threshold][num_topics][topic_num]\n",
    "      if author in df.author.unique():\n",
    "        max_vals = df.groupby('author').max()[f'{topic_num}_relevance']\n",
    "  \n",
    "        to_return.append([f'Topic {topic_num}:', html.Br(), \n",
    "          f'{df[df[f\"{topic_num}_relevance\"] == max_vals.loc[author]][\"title\"].to_list()[0]}', \n",
    "          html.Details([html.Summary('Abstract'), \n",
    "                        html.Div(combined[combined.title == f'{df[df[f\"{topic_num}_relevance\"] == max_vals.loc[author]][\"title\"].to_list()[0]}'].abstract)],\n",
    "                        style={\n",
    "                          'font-size' :'80%',\n",
    "                          'font-family' : 'Verdana'},\n",
    "                        ),\n",
    "          html.Br()])\n",
    "    return list(chain(*to_return)), [{'label' : f'{researcher}', 'value' : f'{researcher}'} for researcher in set(author_list)]\n",
    "\n",
    "  if topic != None and author != None:\n",
    "    df = tops[threshold][num_topics][topic]\n",
    "    df = df[df['author'] == author]\n",
    "    df.sort_values(by=f'{topic}_relevance', ascending=False, inplace=True)\n",
    "    titles = df.head(10)['title'].to_list()\n",
    "    \n",
    "    to_return = [\n",
    "      [f'{i} : {title}', \n",
    "      html.Details([html.Summary('Abstract'), \n",
    "                    html.Div(combined[combined.title == title].abstract)], \n",
    "                    style={\n",
    "                      'font-size' :'80%',\n",
    "                      'font-family' : 'Verdana'}), \n",
    "      html.Br()] for i, title in enumerate(titles)]\n",
    "    return list(chain(*to_return)), [{'label' : f'{researcher}', 'value' : f'{researcher}'} for researcher in tops[threshold][num_topics][topic].author.unique()]\n",
    "    \n",
    "\n",
    "\n",
    "@app.callback(\n",
    "  [Output('graph', 'figure'), Output('dropdown_menu', 'options')],\n",
    "  [Input('graph-dropdown', 'value'), Input('dropdown_menu', 'value'), Input('researcher-dropdown', 'value'), Input('word-search', 'value')],\n",
    "  State('graph', 'figure')\n",
    ")\n",
    "def update_graph(value, topic, author, words, previous_fig):\n",
    "  if len(previous_fig['data'][0]['node']['color']) != value + 108:\n",
    "    figs[threshold][value].update_traces(node = dict(color = ['#666699' for i in range(len(labels[value]))]), link = dict(color = ['rgba(204, 204, 204, .5)' for i in range(len(sources[threshold][value]))]))\n",
    "    return figs[threshold][value], [{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "\n",
    "  if len(words) != 0:\n",
    "    doc_vec = np.zeros((1, len(names)))\n",
    "    for word in words:\n",
    "      doc_vec[0][locations[word]] += 1\n",
    "    relations = np.round(models[f'{value}'].transform(doc_vec), 3).tolist()[0]\n",
    "    opacity = {(i+108) : relation for i, relation in enumerate(relations) if relation > .1}\n",
    "    node_colors = ['#666699' if (i not in opacity.keys()) else f'rgba(255, 255, 0, {opacity[i]})' for i in range(len(labels[value]))]\n",
    "    valid_targets = [positions[value][f'Topic{i-108}'] for i in opacity.keys()]\n",
    "    link_colors = ['rgba(204, 204, 204, .5)' if target not in valid_targets else f'rgba(255, 255, 0, .5)' for target in targets[threshold][value]]\n",
    "    figs[threshold][value].update_traces(node = dict(color = node_colors), link = dict(color = link_colors)),\n",
    "    return figs[threshold][value], [{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "\n",
    "\n",
    "  if topic == None and author == None:\n",
    "    figs[threshold][value].update_traces(node = dict(color = ['#666699' for i in range(len(labels[value]))]), link = dict(color = ['rgba(204, 204, 204, .5)' for i in range(len(sources[threshold][value]))]))\n",
    "    return figs[threshold][value], [{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "  \n",
    "  if topic != None and author == None:\n",
    "    node_colors = ['#666699' if (i != positions[value][f'Topic{topic}']) else '#ffff00' for i in range(len(labels[value]))]\n",
    "    link_colors = ['rgba(204, 204, 204, .5)' if target != positions[value][f'Topic{topic}'] else 'rgba(255, 255, 0, .5)' for target in targets[threshold][value]]\n",
    "    figs[threshold][value].update_traces(node = dict(color = node_colors), link = dict(color = link_colors))\n",
    "    return figs[threshold][value], [{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "\n",
    "  if topic == None and author != None:\n",
    "    node_colors = ['#666699' if (i != positions[value][author]) else '#ffff00' for i in range(len(labels[value]))]\n",
    "    link_colors = ['rgba(204, 204, 204, .5)' if source != positions[value][author] else 'rgba(255, 255, 0, .5)' for source in sources[threshold][value]]\n",
    "    figs[threshold][value].update_traces(node = dict(color = node_colors), link = dict(color = link_colors))\n",
    "    return figs[threshold][value], [{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "\n",
    "  if topic != None and author != None:\n",
    "    node_colors = ['#666699' if (i != positions[value][author] and i != positions[value][f'Topic{topic}']) else '#ffff00' for i in range(len(labels[value]))]\n",
    "    link_colors = ['rgba(204, 204, 204, .5)' if (source != positions[value][author] or target != positions[value][f'Topic{topic}']) else 'rgba(255, 255, 0, .5)' for source, target in zip(sources[threshold][value], targets[threshold][value])]\n",
    "    figs[threshold][value].update_traces(node = dict(color = node_colors), link = dict(color = link_colors))\n",
    "    return figs[threshold][value], [{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "\n",
    "@app.callback(\n",
    "  Output('researcher-dropdown', 'value'),\n",
    "  Input('dropdown_menu', 'value'),\n",
    "  State('dropdown_menu', 'value')\n",
    ")\n",
    "def reset_author(topic, previous):\n",
    "  if topic != previous:\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "app.run_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
