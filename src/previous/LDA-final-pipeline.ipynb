{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/online_lda.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n",
      "[nltk_data] Downloading package wordnet to /Users/brian/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import dimcli\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "import spacy\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "np.random.seed(123)\n",
    "import pickle\n",
    "nltk.download('wordnet')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_hdsi_faculty_updated.csv')\n",
    "authors = df[['authors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#authors.loc[row][list_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\\'affiliations\\': [], \\'corresponding\\': \\'\\', \\'current_organization_id\\': \\'\\', \\'first_name\\': \\'Chen\\', \\'last_name\\': \\'Cai\\', \\'orcid\\': \\'\\', \\'raw_affiliation\\': [], \\'researcher_id\\': \\'\\'}, {\\'affiliations\\': [], \\'corresponding\\': \\'\\', \\'current_organization_id\\': \\'grid.261331.4\\', \\'first_name\\': \\'Woojin\\', \\'last_name\\': \\'Kim\\', \\'orcid\\': \"[\\'0000-0001-8081-5872\\']\", \\'raw_affiliation\\': [], \\'researcher_id\\': \\'ur.07705025375.34\\'}, {\\'affiliations\\': [], \\'corresponding\\': \\'\\', \\'current_organization_id\\': \\'grid.261331.4\\', \\'first_name\\': \\'Facundo\\', \\'last_name\\': \\'Mémoli\\', \\'orcid\\': \\'\\', \\'raw_affiliation\\': [], \\'researcher_id\\': \\'ur.0760761423.71\\'}, {\\'affiliations\\': [], \\'corresponding\\': \\'\\', \\'current_organization_id\\': \\'grid.261331.4\\', \\'first_name\\': \\'Yusu\\', \\'last_name\\': \\'Wang\\', \\'orcid\\': \"[\\'0000-0002-8322-4899\\']\", \\'raw_affiliation\\': [], \\'researcher_id\\': \\'ur.01357524473.18\\'}]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = str(authors.loc[0][0])\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chen Cai\n",
      "\n",
      "Woojin Kim\n",
      "ur.07705025375.34\n",
      "Facundo Mémoli\n",
      "ur.0760761423.71\n",
      "Yusu Wang\n",
      "ur.01357524473.18\n"
     ]
    }
   ],
   "source": [
    "fg = list(eval(test))#[0]['first_name']\n",
    "lis = []\n",
    "lis2 = []\n",
    "for i in fg:\n",
    "    if 'first_name' in i:\n",
    "        first = i['first_name']\n",
    "        last = i['last_name']\n",
    "        full = first + \" \" + last\n",
    "        print(full)\n",
    "        lis.append(full)\n",
    "        ids = i['researcher_id']\n",
    "        print(ids)\n",
    "        lis2.append(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new list to collect names\n",
    "new = []\n",
    "#new list to collect corresponding ids\n",
    "new2 = []\n",
    "#looping through length of author column\n",
    "for i in range(len(authors)):\n",
    "    #turning string of list of dictionaries into list of dictionaries\n",
    "    temp = list(eval(authors.loc[i][0]))\n",
    "    #names\n",
    "    lis = []\n",
    "    #ids\n",
    "    lis2 = []\n",
    "    #looping through the list of dictionaries\n",
    "    for i in temp:\n",
    "        if 'first_name' in i:\n",
    "            first = i['first_name']\n",
    "            last = i['last_name']\n",
    "            #concatenating first and last name\n",
    "            full = first + \" \" + last\n",
    "            lis.append(full)\n",
    "            #print(lis)\n",
    "            ids = i['researcher_id']\n",
    "            lis2.append(ids)\n",
    "        else:\n",
    "            lis.append(i)\n",
    "            lis2.append(i)\n",
    "    new.append(lis)\n",
    "    new2.append(lis2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding new column, \"names,\" to the original dataframe\n",
    "names = pd.Series(new)\n",
    "df['names'] = names.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding new column, \"ids,\" to the original dataframe\n",
    "ids = pd.Series(new2)\n",
    "df['ids'] = ids.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate data by researcher-year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2 = df.explode(['names', 'ids']).reset_index(drop=True)\n",
    "df2 = df.apply(pd.Series.explode).reset_index(drop=True)\n",
    "\n",
    "testing = df2['ids'].value_counts()\n",
    "#print(testing.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdsi = pd.read_csv(\"HDSI.csv\")\n",
    "faculty = hdsi[hdsi['Dimensions ID'] != 'no ID']['Dimensions ID']\n",
    "#manually adding professors since they do not have dimensions ids\n",
    "add = pd.Series(['Aaron McMillan Fraenkel', 'Justin Eldridge'])\n",
    "faculty = list(faculty.append(add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned out all names & ids that do not match our hdsi faculty list\n",
    "df3 = df2[df2.ids.isin(faculty)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>authors_count</th>\n",
       "      <th>category_for</th>\n",
       "      <th>concepts</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>journal.id</th>\n",
       "      <th>journal.title</th>\n",
       "      <th>times_cited</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>names</th>\n",
       "      <th>ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'affiliations': [], 'corresponding': '', 'cu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['space', 'metric spaces']</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>pub.1140323831</td>\n",
       "      <td>jour.1292375</td>\n",
       "      <td>SIAM Journal on Applied Algebra and Geometry</td>\n",
       "      <td>0</td>\n",
       "      <td>Elder-Rule-Staircodes for Augmented Metric Spaces</td>\n",
       "      <td>2021</td>\n",
       "      <td>Yusu Wang</td>\n",
       "      <td>ur.01357524473.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Understanding of neuronal circuitry at cellula...</td>\n",
       "      <td>[{'affiliations': [{'city': 'Cold Spring Harbo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'id': '2208', 'name': '08 Information and Co...</td>\n",
       "      <td>['hybrid architecture', 'semantic segmentation...</td>\n",
       "      <td>2020-09-28</td>\n",
       "      <td>pub.1131237716</td>\n",
       "      <td>jour.1336255</td>\n",
       "      <td>Nature Machine Intelligence</td>\n",
       "      <td>3</td>\n",
       "      <td>Semantic segmentation of microscopic neuroanat...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Yusu Wang</td>\n",
       "      <td>ur.01357524473.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>We study Vietoris–Rips complexes of metric wed...</td>\n",
       "      <td>[{'affiliations': [{'name': 'MOSEK ApS, Copenh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'id': '2201', 'name': '01 Mathematical Scien...</td>\n",
       "      <td>['Vietoris–Rips complexes', 'wedge sum', 'metr...</td>\n",
       "      <td>2020-05-20</td>\n",
       "      <td>pub.1127764757</td>\n",
       "      <td>jour.1290431</td>\n",
       "      <td>Journal of Applied and Computational Topology</td>\n",
       "      <td>5</td>\n",
       "      <td>On homotopy types of Vietoris–Rips complexes o...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Yusu Wang</td>\n",
       "      <td>ur.01357524473.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neuroscientific data analysis has traditionall...</td>\n",
       "      <td>[{'affiliations': [{'city': 'Columbus', 'city_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'id': '3120', 'name': '1109 Neurosciences'},...</td>\n",
       "      <td>['collection of neurons', 'hand-tuned paramete...</td>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>pub.1125823510</td>\n",
       "      <td>jour.1293558</td>\n",
       "      <td>bioRxiv</td>\n",
       "      <td>0</td>\n",
       "      <td>Detection and skeletonization of single neuron...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Yusu Wang</td>\n",
       "      <td>ur.01357524473.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Neuroscientific data analysis has traditionall...</td>\n",
       "      <td>[{'affiliations': [], 'corresponding': '', 'cu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'id': '3120', 'name': '1109 Neurosciences'},...</td>\n",
       "      <td>['collection of neurons', 'hand-tuned paramete...</td>\n",
       "      <td>2020-03-20</td>\n",
       "      <td>pub.1126276621</td>\n",
       "      <td>jour.1371339</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>0</td>\n",
       "      <td>Detection and skeletonization of single neuron...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Yusu Wang</td>\n",
       "      <td>ur.01357524473.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2242</th>\n",
       "      <td>20179</td>\n",
       "      <td>2203</td>\n",
       "      <td>2963037546</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['weyl', 'algorithm', 'typical', 'davis', 'unp...</td>\n",
       "      <td>['Justin Eldridge', 'Mikhail Belkin', 'Yusu Wa...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Perturbation theory', 'Eigenvalues and eigen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>Unperturbed: spectral analysis beyond Davis-Kahan</td>\n",
       "      <td>2018</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2243</th>\n",
       "      <td>20182</td>\n",
       "      <td>2204</td>\n",
       "      <td>2963918728</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['correct', 'imply', 'single', 'nesting', 'two...</td>\n",
       "      <td>['Justin Eldridge', 'Mikhail Belkin', 'Yusu Wa...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Hierarchical clustering', 'Cluster analysis'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>Beyond Hartigan Consistency: Merge Distortion ...</td>\n",
       "      <td>2015</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2244</th>\n",
       "      <td>20185</td>\n",
       "      <td>2205</td>\n",
       "      <td>2963004507</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['correct', 'assumption', 'algorithm', 'produc...</td>\n",
       "      <td>['Justin Eldridge', 'Mikhail Belkin', 'Yusu Wa...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Clustering coefficient', 'Cluster analysis',...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>Graphons, mergeons, and so on!</td>\n",
       "      <td>2016</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2245</th>\n",
       "      <td>20188</td>\n",
       "      <td>2206</td>\n",
       "      <td>2004669647</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['hyper', 'recordings', 'noise', 'means', 'tak...</td>\n",
       "      <td>['Justin Eldridge', 'Alison E Lane', 'Mikhail ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Electroencephalography', 'Autism spectrum di...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>journal of neurodevelopmental disorders</td>\n",
       "      <td>26</td>\n",
       "      <td>Robust features for the automatic identificati...</td>\n",
       "      <td>2014</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2246</th>\n",
       "      <td>20192</td>\n",
       "      <td>2207</td>\n",
       "      <td>1895840270</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['characterize', 'extensions', 'pi', 'singular...</td>\n",
       "      <td>['Aaron McMillan Fraenkel']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Isolated singularity', 'Koszul complex', 'Id...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>arxiv symplectic geometry</td>\n",
       "      <td>1</td>\n",
       "      <td>Extensions of Poisson Structures on Singular H...</td>\n",
       "      <td>2013</td>\n",
       "      <td>Aaron McMillan Fraenkel</td>\n",
       "      <td>Aaron McMillan Fraenkel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2247 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  \\\n",
       "0         3           0             0             0.0   \n",
       "1        15           1             1             1.0   \n",
       "2        24           2             2             2.0   \n",
       "3        35           3             3             3.0   \n",
       "4        46           4             4             4.0   \n",
       "...     ...         ...           ...             ...   \n",
       "2242  20179        2203    2963037546             NaN   \n",
       "2243  20182        2204    2963918728             NaN   \n",
       "2244  20185        2205    2963004507             NaN   \n",
       "2245  20188        2206    2004669647             NaN   \n",
       "2246  20192        2207    1895840270             NaN   \n",
       "\n",
       "                                               abstract  \\\n",
       "0                                                   NaN   \n",
       "1     Understanding of neuronal circuitry at cellula...   \n",
       "2     We study Vietoris–Rips complexes of metric wed...   \n",
       "3     Neuroscientific data analysis has traditionall...   \n",
       "4     Neuroscientific data analysis has traditionall...   \n",
       "...                                                 ...   \n",
       "2242  ['weyl', 'algorithm', 'typical', 'davis', 'unp...   \n",
       "2243  ['correct', 'imply', 'single', 'nesting', 'two...   \n",
       "2244  ['correct', 'assumption', 'algorithm', 'produc...   \n",
       "2245  ['hyper', 'recordings', 'noise', 'means', 'tak...   \n",
       "2246  ['characterize', 'extensions', 'pi', 'singular...   \n",
       "\n",
       "                                                authors  authors_count  \\\n",
       "0     [{'affiliations': [], 'corresponding': '', 'cu...            NaN   \n",
       "1     [{'affiliations': [{'city': 'Cold Spring Harbo...            NaN   \n",
       "2     [{'affiliations': [{'name': 'MOSEK ApS, Copenh...            NaN   \n",
       "3     [{'affiliations': [{'city': 'Columbus', 'city_...            NaN   \n",
       "4     [{'affiliations': [], 'corresponding': '', 'cu...            NaN   \n",
       "...                                                 ...            ...   \n",
       "2242  ['Justin Eldridge', 'Mikhail Belkin', 'Yusu Wa...            3.0   \n",
       "2243  ['Justin Eldridge', 'Mikhail Belkin', 'Yusu Wa...            3.0   \n",
       "2244  ['Justin Eldridge', 'Mikhail Belkin', 'Yusu Wa...            3.0   \n",
       "2245  ['Justin Eldridge', 'Alison E Lane', 'Mikhail ...            4.0   \n",
       "2246                        ['Aaron McMillan Fraenkel']            1.0   \n",
       "\n",
       "                                           category_for  \\\n",
       "0                                                   NaN   \n",
       "1     [{'id': '2208', 'name': '08 Information and Co...   \n",
       "2     [{'id': '2201', 'name': '01 Mathematical Scien...   \n",
       "3     [{'id': '3120', 'name': '1109 Neurosciences'},...   \n",
       "4     [{'id': '3120', 'name': '1109 Neurosciences'},...   \n",
       "...                                                 ...   \n",
       "2242                                                NaN   \n",
       "2243                                                NaN   \n",
       "2244                                                NaN   \n",
       "2245                                                NaN   \n",
       "2246                                                NaN   \n",
       "\n",
       "                                               concepts        date  \\\n",
       "0                            ['space', 'metric spaces']  2021-01-01   \n",
       "1     ['hybrid architecture', 'semantic segmentation...  2020-09-28   \n",
       "2     ['Vietoris–Rips complexes', 'wedge sum', 'metr...  2020-05-20   \n",
       "3     ['collection of neurons', 'hand-tuned paramete...  2020-03-22   \n",
       "4     ['collection of neurons', 'hand-tuned paramete...  2020-03-20   \n",
       "...                                                 ...         ...   \n",
       "2242  ['Perturbation theory', 'Eigenvalues and eigen...         NaN   \n",
       "2243  ['Hierarchical clustering', 'Cluster analysis'...         NaN   \n",
       "2244  ['Clustering coefficient', 'Cluster analysis',...         NaN   \n",
       "2245  ['Electroencephalography', 'Autism spectrum di...         NaN   \n",
       "2246  ['Isolated singularity', 'Koszul complex', 'Id...         NaN   \n",
       "\n",
       "                  id    journal.id  \\\n",
       "0     pub.1140323831  jour.1292375   \n",
       "1     pub.1131237716  jour.1336255   \n",
       "2     pub.1127764757  jour.1290431   \n",
       "3     pub.1125823510  jour.1293558   \n",
       "4     pub.1126276621  jour.1371339   \n",
       "...              ...           ...   \n",
       "2242             NaN           NaN   \n",
       "2243             NaN           NaN   \n",
       "2244             NaN           NaN   \n",
       "2245             NaN           NaN   \n",
       "2246             NaN           NaN   \n",
       "\n",
       "                                      journal.title  times_cited  \\\n",
       "0      SIAM Journal on Applied Algebra and Geometry            0   \n",
       "1                       Nature Machine Intelligence            3   \n",
       "2     Journal of Applied and Computational Topology            5   \n",
       "3                                           bioRxiv            0   \n",
       "4                                             arXiv            0   \n",
       "...                                             ...          ...   \n",
       "2242                                            NaN           50   \n",
       "2243                                            NaN           26   \n",
       "2244                                            NaN           13   \n",
       "2245        journal of neurodevelopmental disorders           26   \n",
       "2246                      arxiv symplectic geometry            1   \n",
       "\n",
       "                                                  title  year  \\\n",
       "0     Elder-Rule-Staircodes for Augmented Metric Spaces  2021   \n",
       "1     Semantic segmentation of microscopic neuroanat...  2020   \n",
       "2     On homotopy types of Vietoris–Rips complexes o...  2020   \n",
       "3     Detection and skeletonization of single neuron...  2020   \n",
       "4     Detection and skeletonization of single neuron...  2020   \n",
       "...                                                 ...   ...   \n",
       "2242  Unperturbed: spectral analysis beyond Davis-Kahan  2018   \n",
       "2243  Beyond Hartigan Consistency: Merge Distortion ...  2015   \n",
       "2244                     Graphons, mergeons, and so on!  2016   \n",
       "2245  Robust features for the automatic identificati...  2014   \n",
       "2246  Extensions of Poisson Structures on Singular H...  2013   \n",
       "\n",
       "                        names                      ids  \n",
       "0                   Yusu Wang        ur.01357524473.18  \n",
       "1                   Yusu Wang        ur.01357524473.18  \n",
       "2                   Yusu Wang        ur.01357524473.18  \n",
       "3                   Yusu Wang        ur.01357524473.18  \n",
       "4                   Yusu Wang        ur.01357524473.18  \n",
       "...                       ...                      ...  \n",
       "2242          Justin Eldridge          Justin Eldridge  \n",
       "2243          Justin Eldridge          Justin Eldridge  \n",
       "2244          Justin Eldridge          Justin Eldridge  \n",
       "2245          Justin Eldridge          Justin Eldridge  \n",
       "2246  Aaron McMillan Fraenkel  Aaron McMillan Fraenkel  \n",
       "\n",
       "[2247 rows x 18 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ur.01357524473.18', 'ur.013664734211.64', 'ur.013314650073.73',\n",
       "       'ur.015666002121.28', 'ur.01101412763.08', 'ur.014341701615.68',\n",
       "       'ur.016311745377.96', 'ur.014432230767.58', 'ur.0700320656.13',\n",
       "       'ur.01305066232.03', 'ur.012616664775.11', 'ur.01154535272.73',\n",
       "       'ur.010412405075.65', 'ur.015225361167.83', 'ur.0762250074.37',\n",
       "       'ur.01135040542.06', 'ur.0645152177.66', 'ur.011716527505.90',\n",
       "       'ur.016136746753.20', 'ur.011713373525.80', 'ur.01362460552.72',\n",
       "       'ur.07600460715.65', 'ur.010165372632.44', 'ur.010063005435.08',\n",
       "       'ur.07640676415.86', 'ur.0743424166.56', 'ur.01227437167.12',\n",
       "       'ur.0626741406.70', 'ur.010352472161.19', 'ur.01344500774.59',\n",
       "       'ur.01112612304.93', 'ur.014372657504.90', 'ur.010103102507.96',\n",
       "       'ur.016005750232.03', 'ur.0640005124.03', 'ur.01075673334.04',\n",
       "       'ur.0676635572.47', 'ur.013410772567.24', 'ur.0624011064.54',\n",
       "       'ur.01040634313.74', 'ur.01164165070.18', 'ur.01211157341.57',\n",
       "       'ur.01365426624.74', 'ur.010341750231.53', 'ur.014531026363.20',\n",
       "       'ur.01013646164.40', 'ur.01315753464.85', 'ur.013751023655.35',\n",
       "       'ur.01307465563.00', 'ur.01353530614.14', 'ur.01143047214.87',\n",
       "       'ur.0645001654.62', 'Justin Eldridge', 'Aaron McMillan Fraenkel'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['ids'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['abstract'] = df3['abstract'].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "redundant = ['abstract', 'purpose', 'paper', 'goal', 'usepackage', 'cod']\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "def preprocess_abstract(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in redundant:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return \" \".join(result)\n",
    "\n",
    "def standardize_abstract(abstract):\n",
    "    abstract = abstract.replace('\\n', ' ')\n",
    "    abstract = abstract.replace('  ', ' ')\n",
    "    abstract = abstract.replace('-', ' ')\n",
    "    abstract = abstract.replace('.', '')\n",
    "    abstract = abstract.replace(':', '')\n",
    "    abstract = abstract.replace(';', '')\n",
    "    abstract = abstract.replace(',', '')\n",
    "    abstract = abstract.replace('\"', '')\n",
    "    abstract = abstract.lower()\n",
    "    return abstract\n",
    "\n",
    "def standardize_title(title):\n",
    "    title = title.replace('\\n', ' ')\n",
    "    title = title.replace('  ', ' ')\n",
    "    title = title.replace('-', ' ')\n",
    "    title = title.replace('.', '')\n",
    "    title = title.replace(':', '')\n",
    "    title = title.replace(';', '')\n",
    "    title = title.replace(',', '')\n",
    "    title = title.replace('\"', '')\n",
    "    title = title.lower()\n",
    "df3['year'] = df3['year'].astype(int)\n",
    "df3['abstract'] = [standardize_abstract(text) for text in df3['abstract']]\n",
    "df3['title_standardized'] = [standardize_title(text) for text in df3['title']]\n",
    "df3['abstract_processed'] = df3['abstract'].apply(preprocess_abstract)\n",
    "df3.drop_duplicates(inplace=True, subset=['abstract'])\n",
    "df3.drop_duplicates(inplace=True, subset=['title_standardized'])\n",
    "df3.drop_duplicates(inplace=True, subset=['abstract_processed'])\n",
    "df3.dropna(axis=0, how='any')\n",
    "df3.reset_index(inplace=True)\n",
    "df3.drop(axis=1, labels=['index'], inplace=True)\n",
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant = ['abstract', 'purpose', 'paper', 'goal', 'usepackage', 'cod']\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "def preprocess_abstract(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in redundant:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return \" \".join(result)\n",
    "\n",
    "\n",
    "df3['abstract_processed'] = df3['abstract'].apply(preprocess_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>authors_count</th>\n",
       "      <th>category_for</th>\n",
       "      <th>concepts</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>journal.id</th>\n",
       "      <th>journal.title</th>\n",
       "      <th>times_cited</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>names</th>\n",
       "      <th>ids</th>\n",
       "      <th>abstract_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>[{'affiliations': [], 'corresponding': '', 'cu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['space', 'metric spaces']</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>pub.1140323831</td>\n",
       "      <td>jour.1292375</td>\n",
       "      <td>SIAM Journal on Applied Algebra and Geometry</td>\n",
       "      <td>0</td>\n",
       "      <td>Elder-Rule-Staircodes for Augmented Metric Spaces</td>\n",
       "      <td>2021</td>\n",
       "      <td>Yusu Wang</td>\n",
       "      <td>ur.01357524473.18</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Understanding of neuronal circuitry at cellula...</td>\n",
       "      <td>[{'affiliations': [{'city': 'Cold Spring Harbo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'id': '2208', 'name': '08 Information and Co...</td>\n",
       "      <td>['hybrid architecture', 'semantic segmentation...</td>\n",
       "      <td>2020-09-28</td>\n",
       "      <td>pub.1131237716</td>\n",
       "      <td>jour.1336255</td>\n",
       "      <td>Nature Machine Intelligence</td>\n",
       "      <td>3</td>\n",
       "      <td>Semantic segmentation of microscopic neuroanat...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Yusu Wang</td>\n",
       "      <td>ur.01357524473.18</td>\n",
       "      <td>understand neuronal circuitry cellular resolut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>We study Vietoris–Rips complexes of metric wed...</td>\n",
       "      <td>[{'affiliations': [{'name': 'MOSEK ApS, Copenh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'id': '2201', 'name': '01 Mathematical Scien...</td>\n",
       "      <td>['Vietoris–Rips complexes', 'wedge sum', 'metr...</td>\n",
       "      <td>2020-05-20</td>\n",
       "      <td>pub.1127764757</td>\n",
       "      <td>jour.1290431</td>\n",
       "      <td>Journal of Applied and Computational Topology</td>\n",
       "      <td>5</td>\n",
       "      <td>On homotopy types of Vietoris–Rips complexes o...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Yusu Wang</td>\n",
       "      <td>ur.01357524473.18</td>\n",
       "      <td>study vietoris rip complexes metric wedge sum ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neuroscientific data analysis has traditionall...</td>\n",
       "      <td>[{'affiliations': [{'city': 'Columbus', 'city_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'id': '3120', 'name': '1109 Neurosciences'},...</td>\n",
       "      <td>['collection of neurons', 'hand-tuned paramete...</td>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>pub.1125823510</td>\n",
       "      <td>jour.1293558</td>\n",
       "      <td>bioRxiv</td>\n",
       "      <td>0</td>\n",
       "      <td>Detection and skeletonization of single neuron...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Yusu Wang</td>\n",
       "      <td>ur.01357524473.18</td>\n",
       "      <td>neuroscientific data analysis traditionally re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Neuroscientific data analysis has traditionall...</td>\n",
       "      <td>[{'affiliations': [], 'corresponding': '', 'cu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'id': '3120', 'name': '1109 Neurosciences'},...</td>\n",
       "      <td>['collection of neurons', 'hand-tuned paramete...</td>\n",
       "      <td>2020-03-20</td>\n",
       "      <td>pub.1126276621</td>\n",
       "      <td>jour.1371339</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>0</td>\n",
       "      <td>Detection and skeletonization of single neuron...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Yusu Wang</td>\n",
       "      <td>ur.01357524473.18</td>\n",
       "      <td>neuroscientific data analysis traditionally re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2242</th>\n",
       "      <td>20179</td>\n",
       "      <td>2203</td>\n",
       "      <td>2963037546</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['weyl', 'algorithm', 'typical', 'davis', 'unp...</td>\n",
       "      <td>['Justin Eldridge', 'Mikhail Belkin', 'Yusu Wa...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Perturbation theory', 'Eigenvalues and eigen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>Unperturbed: spectral analysis beyond Davis-Kahan</td>\n",
       "      <td>2018</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "      <td>weyl algorithm typical davis unperturbed tool ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2243</th>\n",
       "      <td>20182</td>\n",
       "      <td>2204</td>\n",
       "      <td>2963918728</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['correct', 'imply', 'single', 'nesting', 'two...</td>\n",
       "      <td>['Justin Eldridge', 'Mikhail Belkin', 'Yusu Wa...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Hierarchical clustering', 'Cluster analysis'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>Beyond Hartigan Consistency: Merge Distortion ...</td>\n",
       "      <td>2015</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "      <td>correct imply single nest improper view popula...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2244</th>\n",
       "      <td>20185</td>\n",
       "      <td>2205</td>\n",
       "      <td>2963004507</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['correct', 'assumption', 'algorithm', 'produc...</td>\n",
       "      <td>['Justin Eldridge', 'Mikhail Belkin', 'Yusu Wa...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Clustering coefficient', 'Cluster analysis',...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>Graphons, mergeons, and so on!</td>\n",
       "      <td>2016</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "      <td>correct assumption algorithm produce graphons ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2245</th>\n",
       "      <td>20188</td>\n",
       "      <td>2206</td>\n",
       "      <td>2004669647</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['hyper', 'recordings', 'noise', 'means', 'tak...</td>\n",
       "      <td>['Justin Eldridge', 'Alison E Lane', 'Mikhail ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Electroencephalography', 'Autism spectrum di...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>journal of neurodevelopmental disorders</td>\n",
       "      <td>26</td>\n",
       "      <td>Robust features for the automatic identificati...</td>\n",
       "      <td>2014</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "      <td>Justin Eldridge</td>\n",
       "      <td>hyper record noise mean take robust efficacy a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2246</th>\n",
       "      <td>20192</td>\n",
       "      <td>2207</td>\n",
       "      <td>1895840270</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['characterize', 'extensions', 'pi', 'singular...</td>\n",
       "      <td>['Aaron McMillan Fraenkel']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Isolated singularity', 'Koszul complex', 'Id...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>arxiv symplectic geometry</td>\n",
       "      <td>1</td>\n",
       "      <td>Extensions of Poisson Structures on Singular H...</td>\n",
       "      <td>2013</td>\n",
       "      <td>Aaron McMillan Fraenkel</td>\n",
       "      <td>Aaron McMillan Fraenkel</td>\n",
       "      <td>characterize extensions singular koszul singul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2247 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  \\\n",
       "0         3           0             0             0.0   \n",
       "1        15           1             1             1.0   \n",
       "2        24           2             2             2.0   \n",
       "3        35           3             3             3.0   \n",
       "4        46           4             4             4.0   \n",
       "...     ...         ...           ...             ...   \n",
       "2242  20179        2203    2963037546             NaN   \n",
       "2243  20182        2204    2963918728             NaN   \n",
       "2244  20185        2205    2963004507             NaN   \n",
       "2245  20188        2206    2004669647             NaN   \n",
       "2246  20192        2207    1895840270             NaN   \n",
       "\n",
       "                                               abstract  \\\n",
       "0                                                         \n",
       "1     Understanding of neuronal circuitry at cellula...   \n",
       "2     We study Vietoris–Rips complexes of metric wed...   \n",
       "3     Neuroscientific data analysis has traditionall...   \n",
       "4     Neuroscientific data analysis has traditionall...   \n",
       "...                                                 ...   \n",
       "2242  ['weyl', 'algorithm', 'typical', 'davis', 'unp...   \n",
       "2243  ['correct', 'imply', 'single', 'nesting', 'two...   \n",
       "2244  ['correct', 'assumption', 'algorithm', 'produc...   \n",
       "2245  ['hyper', 'recordings', 'noise', 'means', 'tak...   \n",
       "2246  ['characterize', 'extensions', 'pi', 'singular...   \n",
       "\n",
       "                                                authors  authors_count  \\\n",
       "0     [{'affiliations': [], 'corresponding': '', 'cu...            NaN   \n",
       "1     [{'affiliations': [{'city': 'Cold Spring Harbo...            NaN   \n",
       "2     [{'affiliations': [{'name': 'MOSEK ApS, Copenh...            NaN   \n",
       "3     [{'affiliations': [{'city': 'Columbus', 'city_...            NaN   \n",
       "4     [{'affiliations': [], 'corresponding': '', 'cu...            NaN   \n",
       "...                                                 ...            ...   \n",
       "2242  ['Justin Eldridge', 'Mikhail Belkin', 'Yusu Wa...            3.0   \n",
       "2243  ['Justin Eldridge', 'Mikhail Belkin', 'Yusu Wa...            3.0   \n",
       "2244  ['Justin Eldridge', 'Mikhail Belkin', 'Yusu Wa...            3.0   \n",
       "2245  ['Justin Eldridge', 'Alison E Lane', 'Mikhail ...            4.0   \n",
       "2246                        ['Aaron McMillan Fraenkel']            1.0   \n",
       "\n",
       "                                           category_for  \\\n",
       "0                                                   NaN   \n",
       "1     [{'id': '2208', 'name': '08 Information and Co...   \n",
       "2     [{'id': '2201', 'name': '01 Mathematical Scien...   \n",
       "3     [{'id': '3120', 'name': '1109 Neurosciences'},...   \n",
       "4     [{'id': '3120', 'name': '1109 Neurosciences'},...   \n",
       "...                                                 ...   \n",
       "2242                                                NaN   \n",
       "2243                                                NaN   \n",
       "2244                                                NaN   \n",
       "2245                                                NaN   \n",
       "2246                                                NaN   \n",
       "\n",
       "                                               concepts        date  \\\n",
       "0                            ['space', 'metric spaces']  2021-01-01   \n",
       "1     ['hybrid architecture', 'semantic segmentation...  2020-09-28   \n",
       "2     ['Vietoris–Rips complexes', 'wedge sum', 'metr...  2020-05-20   \n",
       "3     ['collection of neurons', 'hand-tuned paramete...  2020-03-22   \n",
       "4     ['collection of neurons', 'hand-tuned paramete...  2020-03-20   \n",
       "...                                                 ...         ...   \n",
       "2242  ['Perturbation theory', 'Eigenvalues and eigen...         NaN   \n",
       "2243  ['Hierarchical clustering', 'Cluster analysis'...         NaN   \n",
       "2244  ['Clustering coefficient', 'Cluster analysis',...         NaN   \n",
       "2245  ['Electroencephalography', 'Autism spectrum di...         NaN   \n",
       "2246  ['Isolated singularity', 'Koszul complex', 'Id...         NaN   \n",
       "\n",
       "                  id    journal.id  \\\n",
       "0     pub.1140323831  jour.1292375   \n",
       "1     pub.1131237716  jour.1336255   \n",
       "2     pub.1127764757  jour.1290431   \n",
       "3     pub.1125823510  jour.1293558   \n",
       "4     pub.1126276621  jour.1371339   \n",
       "...              ...           ...   \n",
       "2242             NaN           NaN   \n",
       "2243             NaN           NaN   \n",
       "2244             NaN           NaN   \n",
       "2245             NaN           NaN   \n",
       "2246             NaN           NaN   \n",
       "\n",
       "                                      journal.title  times_cited  \\\n",
       "0      SIAM Journal on Applied Algebra and Geometry            0   \n",
       "1                       Nature Machine Intelligence            3   \n",
       "2     Journal of Applied and Computational Topology            5   \n",
       "3                                           bioRxiv            0   \n",
       "4                                             arXiv            0   \n",
       "...                                             ...          ...   \n",
       "2242                                            NaN           50   \n",
       "2243                                            NaN           26   \n",
       "2244                                            NaN           13   \n",
       "2245        journal of neurodevelopmental disorders           26   \n",
       "2246                      arxiv symplectic geometry            1   \n",
       "\n",
       "                                                  title  year  \\\n",
       "0     Elder-Rule-Staircodes for Augmented Metric Spaces  2021   \n",
       "1     Semantic segmentation of microscopic neuroanat...  2020   \n",
       "2     On homotopy types of Vietoris–Rips complexes o...  2020   \n",
       "3     Detection and skeletonization of single neuron...  2020   \n",
       "4     Detection and skeletonization of single neuron...  2020   \n",
       "...                                                 ...   ...   \n",
       "2242  Unperturbed: spectral analysis beyond Davis-Kahan  2018   \n",
       "2243  Beyond Hartigan Consistency: Merge Distortion ...  2015   \n",
       "2244                     Graphons, mergeons, and so on!  2016   \n",
       "2245  Robust features for the automatic identificati...  2014   \n",
       "2246  Extensions of Poisson Structures on Singular H...  2013   \n",
       "\n",
       "                        names                      ids  \\\n",
       "0                   Yusu Wang        ur.01357524473.18   \n",
       "1                   Yusu Wang        ur.01357524473.18   \n",
       "2                   Yusu Wang        ur.01357524473.18   \n",
       "3                   Yusu Wang        ur.01357524473.18   \n",
       "4                   Yusu Wang        ur.01357524473.18   \n",
       "...                       ...                      ...   \n",
       "2242          Justin Eldridge          Justin Eldridge   \n",
       "2243          Justin Eldridge          Justin Eldridge   \n",
       "2244          Justin Eldridge          Justin Eldridge   \n",
       "2245          Justin Eldridge          Justin Eldridge   \n",
       "2246  Aaron McMillan Fraenkel  Aaron McMillan Fraenkel   \n",
       "\n",
       "                                     abstract_processed  \n",
       "0                                                        \n",
       "1     understand neuronal circuitry cellular resolut...  \n",
       "2     study vietoris rip complexes metric wedge sum ...  \n",
       "3     neuroscientific data analysis traditionally re...  \n",
       "4     neuroscientific data analysis traditionally re...  \n",
       "...                                                 ...  \n",
       "2242  weyl algorithm typical davis unperturbed tool ...  \n",
       "2243  correct imply single nest improper view popula...  \n",
       "2244  correct assumption algorithm produce graphons ...  \n",
       "2245  hyper record noise mean take robust efficacy a...  \n",
       "2246  characterize extensions singular koszul singul...  \n",
       "\n",
       "[2247 rows x 19 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3[df3['year'] >= 2015]\n",
    "counts = CountVectorizer().fit_transform(df3['abstract_processed'])\n",
    "authors = {}\n",
    "for author in df3.names.unique():\n",
    "    authors[author] = {\n",
    "        2015 : list(),\n",
    "        2016 : list(),\n",
    "        2017 : list(),\n",
    "        2018 : list(),\n",
    "        2019 : list(),\n",
    "        2020 : list(),\n",
    "        2021 : list()\n",
    "    }\n",
    "for i, row in df3.iterrows():\n",
    "    authors[row['names']][row['year']].append(row['abstract_processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_docs = []\n",
    "#for author, author_dict in authors.items():\n",
    "#    for year, documents in author_dict.items():\n",
    "#        all_docs.append(\" \".join(documents))\n",
    "#len(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "381"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs = []\n",
    "missing_author_years = {author : list() for author in df3.names.unique()}\n",
    "for author, author_dict in authors.items():\n",
    "    for year, documents in author_dict.items():\n",
    "        if len(documents) == 0:\n",
    "            missing_author_years[author].append(year)\n",
    "            continue\n",
    "        all_docs.append(\" \".join(documents))\n",
    "len(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initate LDA model\n",
    "countVec = CountVectorizer()\n",
    "counts = countVec.fit_transform(all_docs)\n",
    "names = countVec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n"
     ]
    }
   ],
   "source": [
    "modeller = LatentDirichletAllocation(n_components=10, n_jobs=-1, random_state=123)\n",
    "result = modeller.fit_transform(counts)\n",
    "modeller2 = LatentDirichletAllocation(n_components=20, n_jobs=-1, random_state=123)\n",
    "result2 = modeller2.fit_transform(counts)\n",
    "modeller3 = LatentDirichletAllocation(n_components=30, n_jobs=-1, random_state=123)\n",
    "result3 = modeller3.fit_transform(counts)\n",
    "modeller4 = LatentDirichletAllocation(n_components=40, n_jobs=-1, random_state=123)\n",
    "result4 = modeller4.fit_transform(counts)\n",
    "modeller5 = LatentDirichletAllocation(n_components=50, n_jobs=-1, random_state=123)\n",
    "result5 = modeller5.fit_transform(counts)\n",
    "\n",
    "models = {'10':modeller,'20':modeller2,'30':modeller3,'40':modeller4,'50':modeller5}\n",
    "results = {'10':result,'20':result2,'30':result3,'40':result4,'50':result5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "topicnames = {\n",
    "    num_topics : [\"Topic\" + str(i) for i in range(num_topics)] for num_topics in range(10, 60, 10)\n",
    "}\n",
    "\n",
    "# index names\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(all_docs))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = {\n",
    "    num_topics : pd.DataFrame(results[f'{num_topics}'], columns=topicnames[num_topics], index=docnames) for num_topics in range(10, 60, 10)\n",
    "}\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = {\n",
    "    num_topics : np.argmax(df_document_topic[num_topics].values, axis=1) for num_topics in range(10, 60, 10)\n",
    "}\n",
    "\n",
    "for num_topics, df in df_document_topic.items():\n",
    "    df['dominant_topic'] = dominant_topic[num_topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_list = []\n",
    "year_list = []\n",
    "for author in authors.keys():\n",
    "    for i in range(7):\n",
    "        if (2015 + i) not in missing_author_years[author]:\n",
    "            author_list.append(author)\n",
    "            year_list.append(2015 + i)\n",
    "\n",
    "for df in df_document_topic.values():\n",
    "    df['author'] = author_list\n",
    "    df['year'] = year_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged = {\n",
    "    num_topics : df_document_topic[num_topics].groupby('author').mean().drop(['dominant_topic', 'year'], axis=1) for num_topics in df_document_topic.keys()\n",
    "}\n",
    "\n",
    "filtered = {\n",
    "    threshold : {num_topics : averaged[num_topics].mask(averaged[num_topics] < threshold, other=0) for num_topics in averaged.keys()} for threshold in [.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "for num_topics in range(10, 60, 10):\n",
    "    labels[num_topics] = filtered[.1][num_topics].index.to_list()\n",
    "    labels[num_topics].extend(filtered[.1][num_topics].columns.to_list())\n",
    "\n",
    "\n",
    "sources = {threshold : {} for threshold in [.1]}\n",
    "targets = {threshold : {} for threshold in [.1]}\n",
    "values = {threshold : {} for threshold in [.1]}\n",
    "\n",
    "for threshold in [.1]:\n",
    "    for num_topics in range(10, 60, 10):\n",
    "        curr_sources = []\n",
    "        curr_targets = []\n",
    "        curr_values = []\n",
    "        index_counter = 0\n",
    "        for index, row in filtered[threshold][num_topics].iterrows():\n",
    "            for i, value in enumerate(row):\n",
    "                if value != 0:\n",
    "                    curr_sources.append(index_counter)\n",
    "                    curr_targets.append(108 + i)\n",
    "                    curr_values.append(value)\n",
    "            index_counter += 1\n",
    "        sources[threshold][num_topics] = curr_sources\n",
    "        targets[threshold][num_topics] = curr_targets\n",
    "        values[threshold][num_topics] = curr_values\n",
    "\n",
    "positions = {\n",
    "    num_topics : {label : i for i, label in enumerate(labels[num_topics])} for num_topics in averaged.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_ranks(array):\n",
    "    ranks = []\n",
    "    for value in array:\n",
    "        for i, percentage in enumerate(np.arange(.1, 1.1, .1)):\n",
    "            if value <= np.quantile(array, percentage):\n",
    "                ranks.append(i + 1)\n",
    "                break\n",
    "    return ranks\n",
    "\n",
    "final_values = {threshold : {} for threshold in [.1]}\n",
    "\n",
    "for threshold in [.1]:\n",
    "    for num_topics in range(10, 60, 10):\n",
    "        curr_values_array = np.array(values[threshold][num_topics])\n",
    "        final_values[threshold][num_topics] = split_into_ranks(curr_values_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics_list(model, feature_names, no_top_words):\n",
    "    topic_list = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_list.append(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "    return topic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_labels = {}\n",
    "for num_topics in range(10, 60, 10):\n",
    "    link_labels[num_topics] = labels[num_topics].copy()\n",
    "    link_labels[num_topics][50:] = display_topics_list(models[f'{num_topics}'], names, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n"
     ]
    }
   ],
   "source": [
    "counts = CountVectorizer().fit_transform(df3['abstract_processed'])\n",
    "transformed_list = []\n",
    "for model in models.values():\n",
    "    transformed_list.append(model.transform(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {threshold : {} for threshold in [.1]}\n",
    "for i, matrix in enumerate(transformed_list):\n",
    "    for threshold in [.1]:\n",
    "        df = pd.DataFrame(matrix)\n",
    "        df.mask(df < threshold, other=0, inplace=True)\n",
    "        df['author'] = df3['names']\n",
    "        df['year'] = df3['year']\n",
    "        df['citations'] = df3['times_cited'] + 1\n",
    "\n",
    "        # noralization of citations: Scaling to a range [0, 1]\n",
    "        df['citations_norm'] = df.groupby(by=['author', 'year'])['citations'].apply(lambda x: (x-x.min())/(x.max()-x.min()))#normalize_by_group(df=df, by=['author', 'year'])['citations']\n",
    "        df['abstract'] = df3['abstract']\n",
    "        df['title'] = df3['title']\n",
    "        df.fillna(1, inplace=True)\n",
    "        \n",
    "        #alpha weight parameter for weighting importance of citations vs topic relation\n",
    "        alpha = .75\n",
    "        for topic_num in range((i+1) * 10):\n",
    "            df[f'{topic_num}_relevance'] = alpha * df[topic_num] + (1-alpha) * df['citations_norm']\n",
    "        dataframes[threshold][(i+1) * 10] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([               0,                1,                2,                3,\n",
       "                      4,                5,                6,                7,\n",
       "                      8,                9,         'author',           'year',\n",
       "            'citations', 'citations_norm',       'abstract',          'title',\n",
       "          '0_relevance',    '1_relevance',    '2_relevance',    '3_relevance',\n",
       "          '4_relevance',    '5_relevance',    '6_relevance',    '7_relevance',\n",
       "          '8_relevance',    '9_relevance'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes[0.1][10].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_top_list(data_frame, num_topics, threshold):\n",
    "    top_5s = []\n",
    "    the_filter = filtered[threshold][num_topics]\n",
    "    for topic in range(num_topics):\n",
    "        relevant = the_filter[the_filter[f'Topic{topic}'] != 0].index.to_list()\n",
    "        to_append = data_frame[data_frame[f'{topic}_relevance'] > 0].reset_index()\n",
    "        to_append = to_append[to_append['author'].isin(relevant)].reset_index()\n",
    "        top_5s.append(to_append) \n",
    "    return top_5s\n",
    "\n",
    "tops = {\n",
    "    threshold : {num_topics : create_top_list(dataframes[threshold][num_topics], num_topics, threshold) for num_topics in range(10, 60, 10)} for threshold in [.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: \n",
      "The dash_core_components package is deprecated. Please replace\n",
      "`import dash_core_components as dcc` with `from dash import dcc`\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: \n",
      "The dash_html_components package is deprecated. Please replace\n",
      "`import dash_html_components as html` with `from dash import html`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import dash_bootstrap_components as dbc\n",
    "import plotly.graph_objects as go\n",
    "from dash.dependencies import Input, Output, State\n",
    "\n",
    "# sankey diagrams for diff numbers of topics\n",
    "\n",
    "heights = {\n",
    "  10 : 1000,\n",
    "  20 : 1500,\n",
    "  30 : 2000,\n",
    "  40 : 2500,\n",
    "  50 : 3000\n",
    "}\n",
    "\n",
    "figs = {threshold : {} for threshold in [.1]}\n",
    "for threshold in [.1]:\n",
    "    for num_topics in range(10, 60, 10):\n",
    "        fig = go.Figure(data=[go.Sankey(\n",
    "            node = dict(\n",
    "                pad = 15,\n",
    "                thickness = 20,\n",
    "                line = dict(color = 'black', width = 0.5),\n",
    "                label = labels[num_topics],\n",
    "                color = ['#666699' for i in range(len(labels[num_topics]))],\n",
    "                customdata = link_labels[num_topics],\n",
    "                hovertemplate='%{customdata} Total Flow: %{value}<extra></extra>'\n",
    "            ),\n",
    "            link = dict(\n",
    "                color = ['rgba(204, 204, 204, .5)' for i in range(len(sources[threshold][num_topics]))],\n",
    "                source = sources[threshold][num_topics],\n",
    "                target = targets[threshold][num_topics],\n",
    "                value = final_values[threshold][num_topics]\n",
    "            )\n",
    "        )])\n",
    "        fig.update_layout(title_text=\"Author Topic Connections\", font=dict(size = 10, color = 'white'), height=heights[num_topics], paper_bgcolor=\"black\", plot_bgcolor='black')\n",
    "        figs[threshold][num_topics] = fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A Hsiao',\n",
       " 'A. Cloninger',\n",
       " 'A. Hsiao',\n",
       " 'A. Schwartzman',\n",
       " 'Albert Hsiao',\n",
       " 'Alexander Cloninger',\n",
       " 'Angela J Yu',\n",
       " 'Angela J. Yu',\n",
       " 'Angela Yu',\n",
       " 'Armin Schwartzman',\n",
       " 'Arun Kumar',\n",
       " 'Arya Mazumdar',\n",
       " 'B Smarr',\n",
       " 'Babak Salimi',\n",
       " 'Barna Saha',\n",
       " 'Benjamin L. Smarr',\n",
       " 'Benjamin Smarr',\n",
       " 'Berk Ustun',\n",
       " 'Bradley Voytek',\n",
       " 'D. N. Politis',\n",
       " 'David Danks',\n",
       " 'David J. Danks',\n",
       " 'Dimitris N. Politis',\n",
       " 'Dimitris Politis',\n",
       " 'E.A. Mukamel',\n",
       " 'Eran A Mukamel',\n",
       " 'Eran A. Mukamel',\n",
       " 'Eran Mukamel',\n",
       " 'Ery Arias-Castro',\n",
       " 'F Wuerthwein',\n",
       " 'F. Wuerthwein',\n",
       " 'Frank Wuerthwein',\n",
       " 'G Sugihara',\n",
       " 'GEORGE SUGIHARA',\n",
       " 'Gal Mishne',\n",
       " 'George Sugihara',\n",
       " 'Henrik Christensen',\n",
       " 'Henrik I Christensen',\n",
       " 'Henrik I. Christensen',\n",
       " 'Henrik Iskov Christensen',\n",
       " 'Ilkay Altintas',\n",
       " 'Ilkay Altıntaş',\n",
       " 'Jelena Bradic',\n",
       " 'Jelena Bradić',\n",
       " 'Jingbo Shang',\n",
       " 'Julian J. McAuley',\n",
       " 'Julian McAuley',\n",
       " 'Justin Eldridge',\n",
       " 'Knight R',\n",
       " 'L Ohno-Machado',\n",
       " 'microbiome microbial sample study sequence data diversity associate human microbiota',\n",
       " 'data model patients health flow methods result base risk research',\n",
       " 'data model learn network base information propose cod provide approach',\n",
       " 'network robustness neural model base adversarial certify methods time function',\n",
       " 'cell type brain cells human gene expression single neurons regulatory',\n",
       " 'learn model time network propose approach process base result state',\n",
       " 'patients data study model clinical cancer treatment measure result survival',\n",
       " 'genome data sequence ecdna identify cancer patients high read genes',\n",
       " 'distance graph algorithm result scheme problem bound approximation time measurements',\n",
       " 'model data neural time method brain network base test result']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_labels[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2203    ['weyl', 'algorithm', 'typical', 'davis', 'unp...\n",
       "Name: abstract, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words = {\n",
    "    10 : display_topics_list(models['10'], names, 10),\n",
    "    20 : display_topics_list(models['20'], names, 10),\n",
    "    30 : display_topics_list(models['30'], names, 10),\n",
    "    40 : display_topics_list(models['40'], names, 10),\n",
    "    50 : display_topics_list(models['50'], names, 10)\n",
    "}\n",
    "\n",
    "combined = pd.read_csv('final_hdsi_faculty_updated.csv')\n",
    "combined[combined.title == 'Unperturbed: spectral analysis beyond Davis-Kahan'].abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = {}\n",
    "for i, word in enumerate(names):\n",
    "    locations[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 111)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m111\u001b[0m\n\u001b[0;31m    if topic == None and author == None:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "threshold = .1\n",
    "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.DARKLY])\n",
    "\n",
    "app.layout = html.Div([\n",
    "  dbc.Row([\n",
    "      dcc.Dropdown(\n",
    "        id='graph-dropdown',\n",
    "        placeholder='select number of LDA topics',\n",
    "        options=[{'label' : f'{i} Topic Model', 'value' : i} for i in range(10, 60, 10)],\n",
    "        style={\n",
    "          'color' : 'black',\n",
    "          'background-color' : '#666699',\n",
    "          'width' : '200%',\n",
    "          'align-items' : 'left',\n",
    "          'justify-content' : 'left',\n",
    "          'padding-left' : '15px'\n",
    "        },\n",
    "        value=10\n",
    "      )\n",
    "  ]),\n",
    "  dbc.Row([\n",
    "    dbc.Col(html.Div([\n",
    "      dcc.Graph(\n",
    "        id = 'graph',\n",
    "        figure = figs[.1][10]\n",
    "      )\n",
    "      ],\n",
    "      style={\n",
    "        'height' : '100vh',\n",
    "        'overflow-y' : 'scroll'\n",
    "      }\n",
    "    )\n",
    "    ),\n",
    "      dbc.Col(html.Div([dbc.Col([\n",
    "        dcc.Dropdown(\n",
    "          id='dropdown_menu',\n",
    "          placeholder='Select a topic',\n",
    "          options=[{'label' : f'Topic {topic}: {top_words[10][topic]}', 'value' : topic} for topic in range(10)],\n",
    "          style={\n",
    "            'color' : 'black',\n",
    "            'background-color' : 'white'\n",
    "          }\n",
    "        ),\n",
    "        dcc.Dropdown(\n",
    "          id='researcher-dropdown',\n",
    "          placeholder='Select Researchers',\n",
    "          options=[{'label' : f'{researcher}', 'value' : f'{researcher}'} for researcher in set(author_list)],\n",
    "          style={\n",
    "            'color' : 'black',\n",
    "            'background-color' : 'white'\n",
    "          }\n",
    "        )]),\n",
    "        dbc.Col(\n",
    "          dcc.Dropdown(\n",
    "            id='word-search',\n",
    "            placeholder='Search by word',\n",
    "            options=[{'label' : word, 'value' : word} for word in names],\n",
    "            style={\n",
    "              'color' : 'black',\n",
    "              'background-color' : 'white'\n",
    "            },\n",
    "            value=[],\n",
    "            multi=True\n",
    "          )\n",
    "        ),\n",
    "        html.Div(\n",
    "          id='paper_container', \n",
    "          children=[\n",
    "            html.P(\n",
    "              children=['Top 5 Papers'],\n",
    "              id='titles_and_authors', \n",
    "              draggable=False, \n",
    "              style={\n",
    "                'font-size' :'150%',\n",
    "                'font-family' : 'Verdana'\n",
    "              }\n",
    "            ),\n",
    "          ],\n",
    "        ),\n",
    "      ], \n",
    "        style={\n",
    "          'height' : '100vh',\n",
    "          'overflow-y' : 'scroll'\n",
    "        }\n",
    "      )\n",
    "      )\n",
    "    ]\n",
    "  )]\n",
    ")\n",
    "\n",
    "@app.callback(\n",
    "  Output('titles_and_authors', 'children'),\n",
    "  Output('researcher-dropdown', 'options'),\n",
    "  Input('dropdown_menu', 'value'),\n",
    "  Input('graph-dropdown', 'value'),\n",
    "  Input('researcher-dropdown', 'value'),\n",
    "  Input('word-search', 'value')\n",
    ")\n",
    "def update_p(topic, num_topics, author, words):\n",
    "  if len(words) != 0:\n",
    "    doc_vec = np.zeros((1, len(names)))\n",
    "    for word in words:\n",
    "      doc_vec[0][locations[word]] += 1\n",
    "    relations = np.round(models[f'{num_topics}'].transform(doc_vec), 5).tolist()[0]\n",
    "    pairs = [(i, relation) for i, relation in enumerate(relations)]\n",
    "    pairs.sort(reverse=True, key=lambda x: x[1])\n",
    "    to_return = [[html.Br(), f'Topic{pair[0]}: {pair[1]}', html.Br()] for pair in pairs]\n",
    "    return list(chain(*to_return)), [{'label' : f'{researcher}', 'value' : f'{researcher}'} for researcher in set(author_list)]\n",
    "\n",
    "  if topic == None and author == None:\n",
    "    return ['Make a selection'], [{'label' : f'{researcher}', 'value' : f'{researcher}'} for researcher in set(author_list)]\n",
    "\n",
    "  if topic != None and author == None:\n",
    "    df = tops[threshold][num_topics][topic]\n",
    "    df_authors = df.author.unique()\n",
    "    max_vals = df.groupby('author').max()[f'{topic}_relevance']\n",
    "\n",
    "    to_return = [[f'{name}:', html.Br(), \n",
    "      f'{df[df[f\"{topic}_relevance\"] == max_vals.loc[name]][\"title\"].to_list()[0]}',\n",
    "      html.Details([html.Summary('Abstract'),\n",
    "                    html.Div(combined[combined.title == f'{df[df[f\"{topic}_relevance\"] == max_vals.loc[name]][\"title\"].to_list()[0]}'].abstract)],\n",
    "                    style={\n",
    "                      'font-size' :'80%',\n",
    "                      'font-family' : 'Verdana'}),\n",
    "      html.Br()] for i, name in enumerate(max_vals.index)]\n",
    "    return list(chain(*to_return)), [{'label' : f'{researcher}', 'value' : f'{researcher}'} for researcher in tops[threshold][num_topics][topic].author.unique()]\n",
    "\n",
    "  if topic == None and author != None:\n",
    "    to_return = []\n",
    "    for topic_num in range(num_topics):\n",
    "      df = tops[threshold][num_topics][topic_num]\n",
    "      if author in df.author.unique():\n",
    "        max_vals = df.groupby('author').max()[f'{topic_num}_relevance']\n",
    "  \n",
    "        to_return.append([f'Topic {topic_num}:', html.Br(), \n",
    "          f'{df[df[f\"{topic_num}_relevance\"] == max_vals.loc[author]][\"title\"].to_list()[0]}', \n",
    "          html.Details([html.Summary('Abstract'), \n",
    "                        html.Div(combined[combined.title == f'{df[df[f\"{topic_num}_relevance\"] == max_vals.loc[author]][\"title\"].to_list()[0]}'].abstract)],\n",
    "                        style={\n",
    "                          'font-size' :'80%',\n",
    "                          'font-family' : 'Verdana'},\n",
    "                        ),\n",
    "          html.Br()])\n",
    "    return list(chain(*to_return)), [{'label' : f'{researcher}', 'value' : f'{researcher}'} for researcher in set(author_list)]\n",
    "\n",
    "  if topic != None and author != None:\n",
    "    df = tops[threshold][num_topics][topic]\n",
    "    df = df[df['author'] == author]\n",
    "    df.sort_values(by=f'{topic}_relevance', ascending=False, inplace=True)\n",
    "    titles = df.head(10)['title'].to_list()\n",
    "    \n",
    "    to_return = [\n",
    "      [f'{i} : {title}', \n",
    "      html.Details([html.Summary('Abstract'), \n",
    "                    html.Div(combined[combined.title == title].abstract)], \n",
    "                    style={\n",
    "                      'font-size' :'80%',\n",
    "                      'font-family' : 'Verdana'}), \n",
    "      html.Br()] for i, title in enumerate(titles)]\n",
    "    return list(chain(*to_return)), [{'label' : f'{researcher}', 'value' : f'{researcher}'} for researcher in tops[threshold][num_topics][topic].author.unique()]\n",
    "    \n",
    "\n",
    "\n",
    "@app.callback(\n",
    "  [Output('graph', 'figure'), Output('dropdown_menu', 'options')],\n",
    "  [Input('graph-dropdown', 'value'), Input('dropdown_menu', 'value'), Input('researcher-dropdown', 'value'), Input('word-search', 'value')],\n",
    "  State('graph', 'figure')\n",
    ")\n",
    "def update_graph(value, topic, author, words, previous_fig):\n",
    "  if len(previous_fig['data'][0]['node']['color']) != value + 108:\n",
    "    figs[threshold][value].update_traces(node = dict(color = ['#666699' for i in range(len(labels[value]))]), link = dict(color = ['rgba(204, 204, 204, .5)' for i in range(len(sources[threshold][value]))]))\n",
    "    return figs[threshold][value], [{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "\n",
    "  if len(words) != 0:\n",
    "    doc_vec = np.zeros((1, len(names)))\n",
    "    for word in words:\n",
    "      doc_vec[0][locations[word]] += 1\n",
    "    relations = np.round(models[f'{value}'].transform(doc_vec), 3).tolist()[0]\n",
    "    opacity = {(i+108) : relation for i, relation in enumerate(relations) if relation > .1}\n",
    "    node_colors = ['#666699' if (i not in opacity.keys()) else f'rgba(255, 255, 0, {opacity[i]})' for i in range(len(labels[value]))]\n",
    "    valid_targets = [positions[value][f'Topic{i-108}'] for i in opacity.keys()]\n",
    "    link_colors = ['rgba(204, 204, 204, .5)' if target not in valid_targets else f'rgba(255, 255, 0, .5)' for target in targets[threshold][value]]\n",
    "    figs[threshold][value].update_traces(node = dict(color = node_colors), link = dict(color = link_colors)),\n",
    "    return figs[threshold][value], [{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "\n",
    "\n",
    "  if topic == None and author == None:\n",
    "    figs[threshold][value].update_traces(node = dict(color = ['#666699' for i in range(len(labels[value]))]), link = dict(color = ['rgba(204, 204, 204, .5)' for i in range(len(sources[threshold][value]))]))\n",
    "    return figs[threshold][value], [{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "  \n",
    "  if topic != None and author == None:\n",
    "    node_colors = ['#666699' if (i != positions[value][f'Topic{topic}']) else '#ffff00' for i in range(len(labels[value]))]\n",
    "    link_colors = ['rgba(204, 204, 204, .5)' if target != positions[value][f'Topic{topic}'] else 'rgba(255, 255, 0, .5)' for target in targets[threshold][value]]\n",
    "    figs[threshold][value].update_traces(node = dict(color = node_colors), link = dict(color = link_colors))\n",
    "    return figs[threshold][value], [{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "\n",
    "  if topic == None and author != None:\n",
    "    node_colors = ['#666699' if (i != positions[value][author]) else '#ffff00' for i in range(len(labels[value]))]\n",
    "    link_colors = ['rgba(204, 204, 204, .5)' if source != positions[value][author] else 'rgba(255, 255, 0, .5)' for source in sources[threshold][value]]\n",
    "    figs[threshold][value].update_traces(node = dict(color = node_colors), link = dict(color = link_colors))\n",
    "    return figs[threshold][value], [{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "\n",
    "  if topic != None and author != None:\n",
    "    node_colors = ['#666699' if (i != positions[value][author] and i != positions[value][f'Topic{topic}']) else '#ffff00' for i in range(len(labels[value]))]\n",
    "    link_colors = ['rgba(204, 204, 204, .5)' if (source != positions[value][author] or target != positions[value][f'Topic{topic}']) else 'rgba(255, 255, 0, .5)' for source, target in zip(sources[threshold][value], targets[threshold][value])]\n",
    "    figs[threshold][value].update_traces(node = dict(color = node_colors), link = dict(color = link_colors))\n",
    "    return figs[threshold][value], [{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "\n",
    "@app.callback(\n",
    "  Output('researcher-dropdown', 'value'),\n",
    "  Input('dropdown_menu', 'value'),\n",
    "  State('dropdown_menu', 'value')\n",
    ")\n",
    "def reset_author(topic, previous):\n",
    "  if topic != previous:\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "app.run_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
