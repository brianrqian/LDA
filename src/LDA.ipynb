{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/brian/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import dimcli\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "import spacy\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "np.random.seed(123)\n",
    "import pickle\n",
    "nltk.download('wordnet')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 7 fields in line 13, saw 118\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-74a6d124d1c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#df = pd.read_csv('test/final_hdsi_faculty_updated.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mauthors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'authors'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2054\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2056\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2057\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 7 fields in line 13, saw 118\n"
     ]
    }
   ],
   "source": [
    "#df = pd.read_csv('test/final_hdsi_faculty_updated.csv')\n",
    "df = pd.read_csv('test/test.csv')\n",
    "authors = df[['authors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = str(authors.loc[0][0])\n",
    "fg = list(eval(test))#[0]['first_name']\n",
    "lis = []\n",
    "lis2 = []\n",
    "for i in fg:\n",
    "    if 'first_name' in i:\n",
    "        first = i['first_name']\n",
    "        last = i['last_name']\n",
    "        full = first + \" \" + last\n",
    "        #print(full)\n",
    "        lis.append(full)\n",
    "        ids = i['researcher_id']\n",
    "        #print(ids)\n",
    "        lis2.append(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new list to collect names\n",
    "new = []\n",
    "#new list to collect corresponding ids\n",
    "new2 = []\n",
    "#looping through length of author column\n",
    "for i in range(len(authors)):\n",
    "    #turning string of list of dictionaries into list of dictionaries\n",
    "    temp = list(eval(authors.loc[i][0]))\n",
    "    #names\n",
    "    lis = []\n",
    "    #ids\n",
    "    lis2 = []\n",
    "    #looping through the list of dictionaries\n",
    "    for i in temp:\n",
    "        if 'first_name' in i:\n",
    "            first = i['first_name']\n",
    "            last = i['last_name']\n",
    "            #concatenating first and last name\n",
    "            full = first + \" \" + last\n",
    "            lis.append(full)\n",
    "            #print(lis)\n",
    "            ids = i['researcher_id']\n",
    "            lis2.append(ids)\n",
    "        else:\n",
    "            lis.append(i)\n",
    "            lis2.append(i)\n",
    "    new.append(lis)\n",
    "    new2.append(lis2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding new column, \"names,\" to the original dataframe\n",
    "names = pd.Series(new)\n",
    "df['names'] = names.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding new column, \"ids,\" to the original dataframe\n",
    "ids = pd.Series(new2)\n",
    "df['ids'] = ids.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate data by researcher-year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2 = df.explode(['names', 'ids']).reset_index(drop=True)\n",
    "df2 = df.apply(pd.Series.explode).reset_index(drop=True)\n",
    "\n",
    "testing = df2['ids'].value_counts()\n",
    "#print(testing.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdsi = pd.read_csv(\"model/HDSI.csv\")\n",
    "faculty = hdsi[hdsi['Dimensions ID'] != 'no ID']['Dimensions ID']\n",
    "#manually adding professors since they do not have dimensions ids\n",
    "add = pd.Series(['Aaron McMillan Fraenkel', 'Justin Eldridge'])\n",
    "faculty = list(faculty.append(add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned out all names & ids that do not match our hdsi faculty list\n",
    "df3 = df2[df2.ids.isin(faculty)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df3['ids'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['abstract'] = df3['abstract'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant = ['abstract', 'purpose', 'paper', 'goal', 'usepackage', 'cod']\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "def preprocess_abstract(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in redundant:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return \" \".join(result)\n",
    "\n",
    "\n",
    "df3['abstract_processed'] = df3['abstract'].apply(preprocess_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3[df3['year'] >= 2015]\n",
    "counts = CountVectorizer().fit_transform(df3['abstract_processed'])\n",
    "authors = {}\n",
    "for author in df3.names.unique():\n",
    "    authors[author] = {\n",
    "        2015 : list(),\n",
    "        2016 : list(),\n",
    "        2017 : list(),\n",
    "        2018 : list(),\n",
    "        2019 : list(),\n",
    "        2020 : list(),\n",
    "        2021 : list()\n",
    "    }\n",
    "for i, row in df3.iterrows():\n",
    "    authors[row['names']][row['year']].append(row['abstract_processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "381"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs = []\n",
    "missing_author_years = {author : list() for author in df3.names.unique()}\n",
    "for author, author_dict in authors.items():\n",
    "    for year, documents in author_dict.items():\n",
    "        if len(documents) == 0:\n",
    "            missing_author_years[author].append(year)\n",
    "            continue\n",
    "        all_docs.append(\" \".join(documents))\n",
    "len(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initate LDA model\n",
    "countVec = CountVectorizer()\n",
    "counts = countVec.fit_transform(all_docs)\n",
    "names = countVec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modeller = LatentDirichletAllocation(n_components=10, n_jobs=-1, random_state=123)\n",
    "result = modeller.fit_transform(counts)\n",
    "modeller2 = LatentDirichletAllocation(n_components=20, n_jobs=-1, random_state=123)\n",
    "result2 = modeller2.fit_transform(counts)\n",
    "modeller3 = LatentDirichletAllocation(n_components=30, n_jobs=-1, random_state=123)\n",
    "result3 = modeller3.fit_transform(counts)\n",
    "modeller4 = LatentDirichletAllocation(n_components=40, n_jobs=-1, random_state=123)\n",
    "result4 = modeller4.fit_transform(counts)\n",
    "modeller5 = LatentDirichletAllocation(n_components=50, n_jobs=-1, random_state=123)\n",
    "result5 = modeller5.fit_transform(counts)\n",
    "\n",
    "models = {'10':modeller,'20':modeller2,'30':modeller3,'40':modeller4,'50':modeller5}\n",
    "results = {'10':result,'20':result2,'30':result3,'40':result4,'50':result5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "topicnames = {\n",
    "    num_topics : [\"Topic\" + str(i) for i in range(num_topics)] for num_topics in range(10, 60, 10)\n",
    "}\n",
    "\n",
    "# index names\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(all_docs))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = {\n",
    "    num_topics : pd.DataFrame(results[f'{num_topics}'], columns=topicnames[num_topics], index=docnames) for num_topics in range(10, 60, 10)\n",
    "}\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = {\n",
    "    num_topics : np.argmax(df_document_topic[num_topics].values, axis=1) for num_topics in range(10, 60, 10)\n",
    "}\n",
    "\n",
    "for num_topics, df in df_document_topic.items():\n",
    "    df['dominant_topic'] = dominant_topic[num_topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10: array([2, 2, 8, 8, 2, 0, 2, 2, 1, 9, 1, 7, 9, 5, 2, 2, 2, 2, 9, 9, 8, 9,\n",
       "        9, 8, 8, 8, 1, 1, 9, 1, 2, 2, 2, 9, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2,\n",
       "        9, 9, 2, 9, 9, 9, 5, 9, 3, 3, 5, 5, 8, 9, 0, 4, 2, 2, 2, 2, 2, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 9, 9, 9, 9, 9, 1, 3, 1, 1, 1, 1,\n",
       "        9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 9, 9,\n",
       "        9, 9, 9, 9, 9, 6, 6, 6, 6, 6, 6, 6, 0, 8, 8, 8, 8, 8, 8, 9, 9, 9,\n",
       "        9, 9, 9, 9, 9, 2, 2, 9, 2, 1, 7, 2, 2, 0, 2, 2, 0, 8, 2, 9, 9, 9,\n",
       "        5, 9, 9, 4, 4, 4, 7, 7, 5, 9, 9, 5, 5, 9, 5, 9, 9, 5, 5, 5, 5, 2,\n",
       "        7, 5, 2, 2, 2, 2, 5, 2, 2, 2, 2, 2, 2, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "        8, 2, 0, 0, 0, 0, 0, 5, 5, 5, 2, 2, 2, 5, 5, 5, 5, 5, 5, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 9, 9, 9, 9, 9, 9, 4, 4, 7, 4, 3, 4, 4, 9,\n",
       "        5, 9, 9, 9, 9, 9, 9, 0, 0, 4, 9, 4, 4, 4, 0, 0, 0, 4, 4, 4, 4, 4,\n",
       "        9, 9, 5, 5, 9, 5, 9, 9, 3, 9, 9, 4, 4, 7, 0, 4, 5, 5, 5, 2, 2, 5,\n",
       "        5, 5, 5, 1, 1, 5, 5, 2, 5, 2, 2, 2, 2, 5, 2, 9, 5, 5, 1, 7, 7, 7,\n",
       "        7, 7, 7, 7, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 0, 2, 2, 3, 2, 9,\n",
       "        2, 2, 9, 9, 2, 2, 2, 0, 0, 0, 0, 5, 3, 9, 5, 9, 5, 9, 9, 5, 0, 9,\n",
       "        0, 2, 2, 0, 6, 6, 1, 0, 0, 1, 1, 1, 1, 1, 1, 7, 7, 2, 0, 9, 2, 9,\n",
       "        1, 9, 0, 0, 9, 9, 8]),\n",
       " 20: array([ 2,  2,  2,  8,  2,  0,  2,  2, 13, 18, 12, 18, 18, 18,  2,  2,  2,\n",
       "         2,  2,  2,  9,  9,  2,  9,  8,  8, 16, 19,  9,  1,  5,  2, 14, 14,\n",
       "         2, 15,  0,  0,  0,  0,  0,  0,  2,  2,  2,  2,  2,  1,  2,  1,  2,\n",
       "        14,  3, 15,  3,  5,  8,  1,  0,  4,  2,  2, 15,  2, 17,  0,  0,  0,\n",
       "        10, 17, 12,  0, 15, 15,  0, 17,  2,  2, 13,  9,  9,  2, 12, 12, 12,\n",
       "        12, 16, 16,  9, 15,  7, 16, 17, 17, 16,  2, 14,  1,  2, 17,  1,  1,\n",
       "        17,  1, 18,  9,  4, 16,  2,  2,  9,  9,  9,  9,  9,  6,  6,  6,  6,\n",
       "         6,  6,  6,  0,  8,  8,  8,  8,  2,  2,  7,  5,  9,  5,  9, 11, 11,\n",
       "         2,  2, 15,  9, 17,  1, 12, 17,  9,  0, 15, 17,  0, 15, 18, 10, 10,\n",
       "        10, 10, 10, 10, 16,  4, 16,  4, 17, 16, 15, 16, 16,  7, 14, 16, 14,\n",
       "        14, 19, 19, 19, 19, 19,  9, 19, 19, 19, 19, 19, 19, 17, 17,  2, 18,\n",
       "        18, 17,  1,  9,  9,  9,  9,  1,  9,  9,  9, 13, 18,  0,  0,  0, 18,\n",
       "         0, 15, 19,  5, 18, 17, 17,  2, 15, 15, 15, 15, 10, 16,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  9,  9,  9,  9,  9,  9,  9,  4,  4,  4,\n",
       "         3,  0,  4,  9, 15,  9,  9,  9,  9, 14,  9,  0,  0,  4, 17,  4,  4,\n",
       "         4,  0,  0,  0, 10,  4,  4,  6,  4, 11,  5,  5,  5,  5,  5,  5,  5,\n",
       "         3, 15,  5,  4, 16, 16,  0,  4,  5,  5,  5,  5,  2,  5,  5,  5, 19,\n",
       "         1, 16, 17,  5, 14,  5,  5,  1,  1,  0, 11,  5,  2,  2,  2,  1,  7,\n",
       "        10, 10, 10,  4,  4,  4,  0, 17,  9, 17,  2,  2,  2,  2,  2,  2,  2,\n",
       "        12,  0, 13, 14, 14,  2,  9, 13,  2, 16,  9, 13, 13,  2,  0,  0,  0,\n",
       "         0, 15,  3, 16,  5,  5,  5,  5,  5, 15,  0,  5,  0, 16, 19,  0,  6,\n",
       "        12, 12,  0,  0, 16, 16, 16, 16, 16, 16, 16, 16, 16,  0, 15, 10, 11,\n",
       "        18,  2,  0,  0,  2,  2,  2]),\n",
       " 30: array([29, 29,  8, 29, 29,  0, 24, 24, 21, 21, 12, 21, 21, 18,  2,  2, 11,\n",
       "         2, 21, 21,  9,  2, 11,  8,  8,  8,  5, 11, 21,  1, 29, 24, 24, 24,\n",
       "        24, 24, 10,  0, 10, 10, 10, 10, 16, 21, 24, 21,  2, 21, 24, 24, 11,\n",
       "        24,  3,  3,  5,  5,  8, 21,  0,  1, 15, 15, 24, 24, 24,  0, 20, 25,\n",
       "        18, 17, 25,  0, 25, 15,  0, 17, 24, 24, 13, 24, 24, 24, 12, 12, 12,\n",
       "        16, 16, 25, 27, 15,  7, 16, 23, 23, 16, 16, 14, 24, 29, 17, 26, 26,\n",
       "        26, 26, 18,  2, 22, 21, 21, 21, 21, 21, 21, 21, 21,  6, 19, 19, 19,\n",
       "         6,  6,  6,  0, 21,  8,  8,  8,  2,  2, 23, 14, 21,  9, 21,  9,  9,\n",
       "        21, 24, 27,  9, 24, 16, 20, 16, 29,  0, 15, 29,  0,  8, 18, 23, 23,\n",
       "         9, 10, 10, 15, 20, 20, 16,  6, 17, 16, 26, 16, 16,  7, 14, 16, 14,\n",
       "        14, 11, 11, 11, 24, 24,  6, 24, 29, 24, 24, 24, 11, 16, 18, 18, 18,\n",
       "        18, 17,  1, 24, 21, 21, 24,  1, 21, 21, 21, 13, 18,  0, 10, 10, 10,\n",
       "        10, 15, 29,  5, 18, 29, 18, 29, 21, 21, 29, 29, 29, 10,  0,  0, 10,\n",
       "        10, 10,  0, 10, 25, 10,  0, 21, 21, 21, 21, 21, 21, 28,  0,  7, 25,\n",
       "        10, 17,  3,  9,  4,  9,  9,  9, 15, 14, 28,  0,  0,  4, 17,  4,  4,\n",
       "         4,  0,  0,  0,  9,  4,  4,  6,  4,  9,  9,  9, 29, 29, 29,  9,  9,\n",
       "         3, 29, 25,  4, 16, 16,  0, 20, 24, 24,  5, 24,  2,  5, 24,  5, 24,\n",
       "         5, 23, 17, 24, 15,  5, 24,  7,  1,  0,  4,  5, 24,  2,  2,  1,  7,\n",
       "        17, 10, 20, 20, 20, 20,  0, 29, 29, 29,  2,  2,  2,  2,  2,  2, 21,\n",
       "        16,  0, 14, 14, 14,  2,  9, 21,  2, 16,  9, 21, 21, 24,  0,  0,  0,\n",
       "         0, 25,  3, 16,  5,  3, 28,  9,  3,  9,  0, 24,  0, 29, 29,  0,  6,\n",
       "        20, 25,  0,  0, 16, 16, 16, 16, 16,  1, 16, 16, 16, 25, 25, 24, 11,\n",
       "        18, 11,  0,  0, 24, 24, 21]),\n",
       " 40: array([29, 29, 31, 29, 29,  0, 24,  2, 13, 21, 12, 21, 21, 18,  2,  2,  2,\n",
       "         2, 21, 21,  9,  9, 19,  9,  8,  8,  5, 19, 21,  1, 29, 15, 29, 24,\n",
       "        24, 15, 10, 10, 10, 10, 10, 10, 16, 24, 24, 21,  2,  1,  1,  1, 11,\n",
       "        18,  3,  3,  5,  5, 29,  1,  0,  0, 30, 32, 24, 32, 30,  0, 20, 25,\n",
       "        34, 18, 25,  0, 25, 25,  0, 25, 22, 22, 23, 29, 21,  2, 12, 12, 37,\n",
       "         2, 16, 34, 22, 14,  7, 39, 23, 23, 32,  2, 38, 25, 29, 17, 26, 26,\n",
       "        17, 26, 31, 32,  4, 21, 21, 21, 21, 35, 21, 35, 35,  6, 37, 37, 19,\n",
       "        37,  6,  6,  0, 21, 21, 21, 21, 21,  2, 23, 11, 21, 29, 21,  9,  9,\n",
       "        39, 39, 39, 39, 24,  1, 12, 35, 26,  0, 19, 29,  0,  8, 18, 15, 15,\n",
       "        15, 15, 10, 15, 16, 14, 16,  6, 24, 16, 36, 34, 16, 16, 14, 16, 14,\n",
       "        14, 32, 32, 32, 32, 32, 10, 32, 29, 32, 32, 32, 32, 18, 18, 18, 18,\n",
       "        18, 17, 30,  9, 21,  9, 38, 30, 35, 21, 21, 13, 18, 10,  0, 10, 18,\n",
       "        10, 15, 34,  5, 18, 18, 36, 29, 21, 21, 29, 29, 29, 16,  0,  0, 10,\n",
       "        25, 10,  0, 13, 25, 10,  0, 34, 34, 34, 34, 34, 21, 28,  0,  7, 20,\n",
       "        10,  0, 33,  9,  4, 35,  9,  9, 21, 14, 35,  0,  0,  4, 30,  4,  4,\n",
       "         4,  0,  0,  0, 10,  4,  4,  6,  4, 11, 11,  9, 29, 29, 29, 27,  9,\n",
       "         3, 29, 25,  4, 16, 16,  0, 10, 24, 24, 24, 24,  2, 24,  5, 24, 29,\n",
       "        32, 32, 27, 24, 38,  5, 27, 32,  1, 24, 36,  5, 24, 15, 15,  1, 32,\n",
       "        10, 30, 28, 20, 20, 29,  0, 36, 36, 36, 15, 15, 15,  2,  2, 21, 21,\n",
       "        20,  0, 27, 24, 31,  2,  9, 27,  2, 16,  9, 27, 27, 18,  0,  0,  0,\n",
       "         0, 15, 36, 16,  5, 24, 34, 34,  3, 35,  0, 24,  0, 29, 29,  0, 14,\n",
       "        30, 25,  0,  0, 16, 36, 16, 16, 16,  1, 16, 16, 36, 35, 28, 24, 11,\n",
       "        18, 24,  0,  0, 21, 21,  2]),\n",
       " 50: array([29, 29, 31, 29, 29,  0, 24, 24, 21, 21, 47, 21, 21, 43,  2,  2,  2,\n",
       "         2, 21, 21,  9,  9, 35,  9,  8,  8, 36, 36, 49,  1, 24, 24, 24, 24,\n",
       "        24, 24, 47, 47, 10, 10, 10, 47, 16, 24, 21, 21, 24,  1, 39, 39,  5,\n",
       "        41,  3,  3,  5,  3,  8, 21,  0, 31, 32, 32, 24, 32, 30,  0, 20, 25,\n",
       "        34, 17, 25,  0, 15, 15,  0, 17, 14, 14, 23, 21, 21,  2, 12, 12, 15,\n",
       "        16, 16, 47, 35, 40, 40, 40, 43, 43, 40, 18, 38,  1, 17, 17, 26, 26,\n",
       "        17, 20, 30, 32, 16, 21, 21, 21, 21, 21, 21, 21, 21,  6, 19, 25, 25,\n",
       "        37,  6,  6,  0, 41, 41,  8,  8, 41,  2, 23, 45, 21, 39, 21, 39, 39,\n",
       "        21, 39, 39,  9, 43,  1, 20, 30, 19,  0, 19, 30,  0, 33, 17, 38, 38,\n",
       "        38, 10, 10, 38, 42, 30, 16, 42, 30, 16, 20, 48, 44, 30, 14, 44, 14,\n",
       "        14, 41, 32, 41, 43, 24,  7, 43, 29, 32, 43, 43, 41, 43, 43, 43, 43,\n",
       "        43, 43, 49, 49, 21, 49, 49, 49, 21, 21, 21, 13, 43, 22, 47, 10, 18,\n",
       "        10, 29, 23, 48, 43, 43, 43, 29, 21, 21, 29, 29, 29, 19,  0,  0, 47,\n",
       "        47, 10,  0, 22, 47, 47,  0, 34, 34, 34, 34, 34, 34, 28, 47,  7, 47,\n",
       "        47, 47, 47, 48,  4, 48,  9,  9, 21, 20, 48,  0,  0, 18, 30,  4, 46,\n",
       "        46,  0,  0,  0, 45, 46, 46, 35, 46, 11, 29, 29, 29, 29, 29, 29, 48,\n",
       "        48, 29, 25,  5, 16, 42,  0, 42, 24, 24, 24, 30, 30, 43, 24, 49, 24,\n",
       "        32, 40, 17, 32, 38, 20, 24, 49,  1, 49, 49,  5, 24,  5,  5,  1, 32,\n",
       "        42, 42, 42, 42, 42, 42,  0,  9,  9, 43,  2,  2,  2,  2,  2,  4,  2,\n",
       "        16,  0, 24, 24, 21,  2,  9, 27,  2, 16,  9, 27, 27, 24,  0,  0,  0,\n",
       "         0, 43,  3, 29,  5, 24, 34, 39, 24,  5,  0, 24,  0, 43, 43,  0, 16,\n",
       "        47, 15,  0,  0, 40, 40, 40, 40, 40,  1, 40, 40, 40, 25, 28, 24, 48,\n",
       "        42, 24,  0,  0, 39, 35, 21])}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dominant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_list = []\n",
    "year_list = []\n",
    "for author in authors.keys():\n",
    "    for i in range(7):\n",
    "        if (2015 + i) not in missing_author_years[author]:\n",
    "            author_list.append(author)\n",
    "            year_list.append(2015 + i)\n",
    "\n",
    "for df in df_document_topic.values():\n",
    "    df['author'] = author_list\n",
    "    df['year'] = year_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged = {\n",
    "    num_topics : df_document_topic[num_topics].groupby('author').mean().drop(['dominant_topic', 'year'], axis=1) for num_topics in df_document_topic.keys()\n",
    "}\n",
    "\n",
    "filtered = {\n",
    "    threshold : {num_topics : averaged[num_topics].mask(averaged[num_topics] < threshold, other=0) for num_topics in averaged.keys()} for threshold in [.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "for num_topics in range(10, 60, 10):\n",
    "    labels[num_topics] = filtered[.1][num_topics].index.to_list()\n",
    "    labels[num_topics].extend(filtered[.1][num_topics].columns.to_list())\n",
    "\n",
    "\n",
    "sources = {threshold : {} for threshold in [.1]}\n",
    "targets = {threshold : {} for threshold in [.1]}\n",
    "values = {threshold : {} for threshold in [.1]}\n",
    "\n",
    "for threshold in [.1]:\n",
    "    for num_topics in range(10, 60, 10):\n",
    "        curr_sources = []\n",
    "        curr_targets = []\n",
    "        curr_values = []\n",
    "        index_counter = 0\n",
    "        for index, row in filtered[threshold][num_topics].iterrows():\n",
    "            for i, value in enumerate(row):\n",
    "                if value != 0:\n",
    "                    curr_sources.append(index_counter)\n",
    "                    curr_targets.append(108 + i)\n",
    "                    curr_values.append(value)\n",
    "            index_counter += 1\n",
    "        sources[threshold][num_topics] = curr_sources\n",
    "        targets[threshold][num_topics] = curr_targets\n",
    "        values[threshold][num_topics] = curr_values\n",
    "\n",
    "positions = {\n",
    "    num_topics : {label : i for i, label in enumerate(labels[num_topics])} for num_topics in averaged.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_ranks(array):\n",
    "    ranks = []\n",
    "    for value in array:\n",
    "        for i, percentage in enumerate(np.arange(.1, 1.1, .1)):\n",
    "            if value <= np.quantile(array, percentage):\n",
    "                ranks.append(i + 1)\n",
    "                break\n",
    "    return ranks\n",
    "\n",
    "final_values = {threshold : {} for threshold in [.1]}\n",
    "\n",
    "for threshold in [.1]:\n",
    "    for num_topics in range(10, 60, 10):\n",
    "        curr_values_array = np.array(values[threshold][num_topics])\n",
    "        final_values[threshold][num_topics] = split_into_ranks(curr_values_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics_list(model, feature_names, no_top_words):\n",
    "    topic_list = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_list.append(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "    return topic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_labels = {}\n",
    "for num_topics in range(10, 60, 10):\n",
    "    link_labels[num_topics] = labels[num_topics].copy()\n",
    "    link_labels[num_topics][50:] = display_topics_list(models[f'{num_topics}'], names, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n"
     ]
    }
   ],
   "source": [
    "counts = CountVectorizer().fit_transform(df3['abstract_processed'])\n",
    "transformed_list = []\n",
    "for model in models.values():\n",
    "    transformed_list.append(model.transform(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {threshold : {} for threshold in [.1]}\n",
    "for i, matrix in enumerate(transformed_list):\n",
    "    for threshold in [.1]:\n",
    "        df = pd.DataFrame(matrix)\n",
    "        df.mask(df < threshold, other=0, inplace=True)\n",
    "        df['author'] = df3['names']\n",
    "        df['year'] = df3['year']\n",
    "        df['citations'] = df3['times_cited'] + 1\n",
    "\n",
    "        # noralization of citations: Scaling to a range [0, 1]\n",
    "        df['citations_norm'] = df.groupby(by=['author', 'year'])['citations'].apply(lambda x: (x-x.min())/(x.max()-x.min()))#normalize_by_group(df=df, by=['author', 'year'])['citations']\n",
    "        df['abstract'] = df3['abstract']\n",
    "        df['title'] = df3['title']\n",
    "        df.fillna(1, inplace=True)\n",
    "        \n",
    "        #alpha weight parameter for weighting importance of citations vs topic relation\n",
    "        alpha = .75\n",
    "        for topic_num in range((i+1) * 10):\n",
    "            df[f'{topic_num}_relevance'] = alpha * df[topic_num] + (1-alpha) * df['citations_norm']\n",
    "        dataframes[threshold][(i+1) * 10] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_top_list(data_frame, num_topics, threshold):\n",
    "    top_5s = []\n",
    "    the_filter = filtered[threshold][num_topics]\n",
    "    for topic in range(num_topics):\n",
    "        relevant = the_filter[the_filter[f'Topic{topic}'] != 0].index.to_list()\n",
    "        to_append = data_frame[data_frame[f'{topic}_relevance'] > 0].reset_index()\n",
    "        to_append = to_append[to_append['author'].isin(relevant)].reset_index()\n",
    "        top_5s.append(to_append) \n",
    "    return top_5s\n",
    "\n",
    "tops = {\n",
    "    threshold : {num_topics : create_top_list(dataframes[threshold][num_topics], num_topics, threshold) for num_topics in range(10, 60, 10)} for threshold in [.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: \n",
      "The dash_core_components package is deprecated. Please replace\n",
      "`import dash_core_components as dcc` with `from dash import dcc`\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: \n",
      "The dash_html_components package is deprecated. Please replace\n",
      "`import dash_html_components as html` with `from dash import html`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import dash_bootstrap_components as dbc\n",
    "import plotly.graph_objects as go\n",
    "from dash.dependencies import Input, Output, State\n",
    "\n",
    "# sankey diagrams for diff numbers of topics\n",
    "\n",
    "heights = {\n",
    "  10 : 1000,\n",
    "  20 : 1500,\n",
    "  30 : 2000,\n",
    "  40 : 2500,\n",
    "  50 : 3000\n",
    "}\n",
    "\n",
    "figs = {threshold : {} for threshold in [.1]}\n",
    "for threshold in [.1]:\n",
    "    for num_topics in range(10, 60, 10):\n",
    "        fig = go.Figure(data=[go.Sankey(\n",
    "            node = dict(\n",
    "                pad = 15,\n",
    "                thickness = 20,\n",
    "                line = dict(color = 'black', width = 0.5),\n",
    "                label = labels[num_topics],\n",
    "                color = ['#666699' for i in range(len(labels[num_topics]))],\n",
    "                customdata = link_labels[num_topics],\n",
    "                hovertemplate='%{customdata} Total Flow: %{value}<extra></extra>'\n",
    "            ),\n",
    "            link = dict(\n",
    "                color = ['rgba(204, 204, 204, .5)' for i in range(len(sources[threshold][num_topics]))],\n",
    "                source = sources[threshold][num_topics],\n",
    "                target = targets[threshold][num_topics],\n",
    "                value = final_values[threshold][num_topics]\n",
    "            )\n",
    "        )])\n",
    "        fig.update_layout(title_text=\"Author Topic Connections\", font=dict(size = 10, color = 'white'), height=heights[num_topics], paper_bgcolor=\"black\", plot_bgcolor='black')\n",
    "        figs[threshold][num_topics] = fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = {\n",
    "    10 : display_topics_list(models['10'], names, 10),\n",
    "    20 : display_topics_list(models['20'], names, 10),\n",
    "    30 : display_topics_list(models['30'], names, 10),\n",
    "    40 : display_topics_list(models['40'], names, 10),\n",
    "    50 : display_topics_list(models['50'], names, 10)\n",
    "}\n",
    "\n",
    "combined = pd.read_csv('model/final_hdsi_faculty_updated.csv')\n",
    "#combined[combined.title == 'Unperturbed: spectral analysis beyond Davis-Kahan'].abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = {}\n",
    "for i, word in enumerate(names):\n",
    "    locations[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "threshold = .1\n",
    "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.DARKLY])\n",
    "\n",
    "app.layout = html.Div([\n",
    "  dbc.Row([\n",
    "      dcc.Dropdown(\n",
    "        id='graph-dropdown',\n",
    "        placeholder='select number of LDA topics',\n",
    "        options=[{'label' : f'{i} Topic Model', 'value' : i} for i in range(10, 60, 10)],\n",
    "        style={\n",
    "          'color' : 'black',\n",
    "          'background-color' : '#666699',\n",
    "          'width' : '200%',\n",
    "          'align-items' : 'left',\n",
    "          'justify-content' : 'left',\n",
    "          'padding-left' : '15px'\n",
    "        },\n",
    "        value=10\n",
    "      )\n",
    "  ]),\n",
    "  dbc.Row([\n",
    "    dbc.Col(html.Div([\n",
    "      dcc.Graph(\n",
    "        id = 'graph',\n",
    "        figure = figs[.1][10]\n",
    "      )\n",
    "      ],\n",
    "      style={\n",
    "        'height' : '100vh',\n",
    "        'overflow-y' : 'scroll'\n",
    "      }\n",
    "    )\n",
    "    ),\n",
    "      dbc.Col(html.Div([dbc.Col([\n",
    "        dcc.Dropdown(\n",
    "          id='dropdown_menu',\n",
    "          placeholder='Select a topic',\n",
    "          options=[{'label' : f'Topic {topic}: {top_words[10][topic]}', 'value' : topic} for topic in range(10)],\n",
    "          style={\n",
    "            'color' : 'black',\n",
    "            'background-color' : 'white'\n",
    "          }\n",
    "        ),\n",
    "        dcc.Dropdown(\n",
    "          id='researcher-dropdown',\n",
    "          placeholder='Select Researchers',\n",
    "          options=[{'label' : f'{researcher}', 'value' : f'{researcher}'} for researcher in set(author_list)],\n",
    "          style={\n",
    "            'color' : 'black',\n",
    "            'background-color' : 'white'\n",
    "          }\n",
    "        )]),\n",
    "        dbc.Col(\n",
    "          dcc.Dropdown(\n",
    "            id='word-search',\n",
    "            placeholder='Search by word',\n",
    "            options=[{'label' : word, 'value' : word} for word in names],\n",
    "            style={\n",
    "              'color' : 'black',\n",
    "              'background-color' : 'white'\n",
    "            },\n",
    "            value=[],\n",
    "            multi=True\n",
    "          )\n",
    "        ),\n",
    "        html.Div(\n",
    "          id='paper_container', \n",
    "          children=[\n",
    "            html.P(\n",
    "              children=['Top 5 Papers'],\n",
    "              id='titles_and_authors', \n",
    "              draggable=False, \n",
    "              style={\n",
    "                'font-size' :'150%',\n",
    "                'font-family' : 'Verdana'\n",
    "              }\n",
    "            ),\n",
    "          ],\n",
    "        ),\n",
    "      ], \n",
    "        style={\n",
    "          'height' : '100vh',\n",
    "          'overflow-y' : 'scroll'\n",
    "        }\n",
    "      )\n",
    "      )\n",
    "    ]\n",
    "  )]\n",
    ")\n",
    "\n",
    "@app.callback(\n",
    "  Output('titles_and_authors', 'children'),\n",
    "  Output('researcher-dropdown', 'options'),\n",
    "  Input('dropdown_menu', 'value'),\n",
    "  Input('graph-dropdown', 'value'),\n",
    "  Input('researcher-dropdown', 'value'),\n",
    "  Input('word-search', 'value')\n",
    ")\n",
    "def update_p(topic, num_topics, author, words):\n",
    "  if len(words) != 0:\n",
    "    doc_vec = np.zeros((1, len(names)))\n",
    "    for word in words:\n",
    "      doc_vec[0][locations[word]] += 1\n",
    "    relations = np.round(models[f'{num_topics}'].transform(doc_vec), 5).tolist()[0]\n",
    "    pairs = [(i, relation) for i, relation in enumerate(relations)]\n",
    "    pairs.sort(reverse=True, key=lambda x: x[1])\n",
    "    to_return = [[html.Br(), f'Topic{pair[0]}: {pair[1]}', html.Br()] for pair in pairs]\n",
    "    return list(chain(*to_return)), [{'label' : f'{researcher}', 'value' : f'{researcher}'} for researcher in set(author_list)]\n",
    "\n",
    "  if topic == None and author == None:\n",
    "    return ['Make a selection'], [{'label' : f'{researcher}', 'value' : f'{researcher}'} for researcher in set(author_list)]\n",
    "\n",
    "  if topic != None and author == None:\n",
    "    df = tops[threshold][num_topics][topic]\n",
    "    df_authors = df.author.unique()\n",
    "    max_vals = df.groupby('author').max()[f'{topic}_relevance']\n",
    "\n",
    "    to_return = [[f'{name}:', html.Br(), \n",
    "      f'{df[df[f\"{topic}_relevance\"] == max_vals.loc[name]][\"title\"].to_list()[0]}',\n",
    "      html.Details([html.Summary('Abstract'),\n",
    "                    html.Div(combined[combined.title == f'{df[df[f\"{topic}_relevance\"] == max_vals.loc[name]][\"title\"].to_list()[0]}'].abstract)],\n",
    "                    style={\n",
    "                      'font-size' :'80%',\n",
    "                      'font-family' : 'Verdana'}),\n",
    "      html.Br()] for i, name in enumerate(max_vals.index)]\n",
    "    return list(chain(*to_return)), [{'label' : f'{researcher}', 'value' : f'{researcher}'} for researcher in tops[threshold][num_topics][topic].author.unique()]\n",
    "\n",
    "  if topic == None and author != None:\n",
    "    to_return = []\n",
    "    for topic_num in range(num_topics):\n",
    "      df = tops[threshold][num_topics][topic_num]\n",
    "      if author in df.author.unique():\n",
    "        max_vals = df.groupby('author').max()[f'{topic_num}_relevance']\n",
    "  \n",
    "        to_return.append([f'Topic {topic_num}:', html.Br(), \n",
    "          f'{df[df[f\"{topic_num}_relevance\"] == max_vals.loc[author]][\"title\"].to_list()[0]}', \n",
    "          html.Details([html.Summary('Abstract'), \n",
    "                        html.Div(combined[combined.title == f'{df[df[f\"{topic_num}_relevance\"] == max_vals.loc[author]][\"title\"].to_list()[0]}'].abstract)],\n",
    "                        style={\n",
    "                          'font-size' :'80%',\n",
    "                          'font-family' : 'Verdana'},\n",
    "                        ),\n",
    "          html.Br()])\n",
    "    return list(chain(*to_return)), [{'label' : f'{researcher}', 'value' : f'{researcher}'} for researcher in set(author_list)]\n",
    "\n",
    "  if topic != None and author != None:\n",
    "    df = tops[threshold][num_topics][topic]\n",
    "    df = df[df['author'] == author]\n",
    "    df.sort_values(by=f'{topic}_relevance', ascending=False, inplace=True)\n",
    "    titles = df.head(10)['title'].to_list()\n",
    "    \n",
    "    to_return = [\n",
    "      [f'{i} : {title}', \n",
    "      html.Details([html.Summary('Abstract'), \n",
    "                    html.Div(combined[combined.title == title].abstract)], \n",
    "                    style={\n",
    "                      'font-size' :'80%',\n",
    "                      'font-family' : 'Verdana'}), \n",
    "      html.Br()] for i, title in enumerate(titles)]\n",
    "    return list(chain(*to_return)), [{'label' : f'{researcher}', 'value' : f'{researcher}'} for researcher in tops[threshold][num_topics][topic].author.unique()]\n",
    "    \n",
    "\n",
    "\n",
    "@app.callback(\n",
    "  [Output('graph', 'figure'), Output('dropdown_menu', 'options')],\n",
    "  [Input('graph-dropdown', 'value'), Input('dropdown_menu', 'value'), Input('researcher-dropdown', 'value'), Input('word-search', 'value')],\n",
    "  State('graph', 'figure')\n",
    ")\n",
    "def update_graph(value, topic, author, words, previous_fig):\n",
    "  if len(previous_fig['data'][0]['node']['color']) != value + 108:\n",
    "    figs[threshold][value].update_traces(node = dict(color = ['#666699' for i in range(len(labels[value]))]), link = dict(color = ['rgba(204, 204, 204, .5)' for i in range(len(sources[threshold][value]))]))\n",
    "    return figs[threshold][value], [{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "\n",
    "  if len(words) != 0:\n",
    "    doc_vec = np.zeros((1, len(names)))\n",
    "    for word in words:\n",
    "      doc_vec[0][locations[word]] += 1\n",
    "    relations = np.round(models[f'{value}'].transform(doc_vec), 3).tolist()[0]\n",
    "    opacity = {(i+108) : relation for i, relation in enumerate(relations) if relation > .1}\n",
    "    node_colors = ['#666699' if (i not in opacity.keys()) else f'rgba(255, 255, 0, {opacity[i]})' for i in range(len(labels[value]))]\n",
    "    valid_targets = [positions[value][f'Topic{i-108}'] for i in opacity.keys()]\n",
    "    link_colors = ['rgba(204, 204, 204, .5)' if target not in valid_targets else f'rgba(255, 255, 0, .5)' for target in targets[threshold][value]]\n",
    "    figs[threshold][value].update_traces(node = dict(color = node_colors), link = dict(color = link_colors)),\n",
    "    return figs[threshold][value], [{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "\n",
    "\n",
    "  if topic == None and author == None:\n",
    "    figs[threshold][value].update_traces(node = dict(color = ['#666699' for i in range(len(labels[value]))]), link = dict(color = ['rgba(204, 204, 204, .5)' for i in range(len(sources[threshold][value]))]))\n",
    "    return figs[threshold][value], [{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "  \n",
    "  if topic != None and author == None:\n",
    "    node_colors = ['#666699' if (i != positions[value][f'Topic{topic}']) else '#ffff00' for i in range(len(labels[value]))]\n",
    "    link_colors = ['rgba(204, 204, 204, .5)' if target != positions[value][f'Topic{topic}'] else 'rgba(255, 255, 0, .5)' for target in targets[threshold][value]]\n",
    "    figs[threshold][value].update_traces(node = dict(color = node_colors), link = dict(color = link_colors))\n",
    "    return figs[threshold][value], [{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "\n",
    "  if topic == None and author != None:\n",
    "    node_colors = ['#666699' if (i != positions[value][author]) else '#ffff00' for i in range(len(labels[value]))]\n",
    "    link_colors = ['rgba(204, 204, 204, .5)' if source != positions[value][author] else 'rgba(255, 255, 0, .5)' for source in sources[threshold][value]]\n",
    "    figs[threshold][value].update_traces(node = dict(color = node_colors), link = dict(color = link_colors))\n",
    "    return figs[threshold][value], [{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "\n",
    "  if topic != None and author != None:\n",
    "    node_colors = ['#666699' if (i != positions[value][author] and i != positions[value][f'Topic{topic}']) else '#ffff00' for i in range(len(labels[value]))]\n",
    "    link_colors = ['rgba(204, 204, 204, .5)' if (source != positions[value][author] or target != positions[value][f'Topic{topic}']) else 'rgba(255, 255, 0, .5)' for source, target in zip(sources[threshold][value], targets[threshold][value])]\n",
    "    figs[threshold][value].update_traces(node = dict(color = node_colors), link = dict(color = link_colors))\n",
    "    return figs[threshold][value], [{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "\n",
    "@app.callback(\n",
    "  Output('researcher-dropdown', 'value'),\n",
    "  Input('dropdown_menu', 'value'),\n",
    "  State('dropdown_menu', 'value')\n",
    ")\n",
    "def reset_author(topic, previous):\n",
    "  if topic != previous:\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "app.run_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
