{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/online_lda.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n",
      "[nltk_data] Downloading package wordnet to /Users/brian/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import dimcli\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "import spacy\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "np.random.seed(123)\n",
    "import pickle\n",
    "nltk.download('wordnet')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_hdsi_faculty_updated.csv')\n",
    "authors = df[['authors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = str(authors.loc[0][0])\n",
    "fg = list(eval(test))#[0]['first_name']\n",
    "lis = []\n",
    "lis2 = []\n",
    "for i in fg:\n",
    "    if 'first_name' in i:\n",
    "        first = i['first_name']\n",
    "        last = i['last_name']\n",
    "        full = first + \" \" + last\n",
    "        #print(full)\n",
    "        lis.append(full)\n",
    "        ids = i['researcher_id']\n",
    "        #print(ids)\n",
    "        lis2.append(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new list to collect names\n",
    "new = []\n",
    "#new list to collect corresponding ids\n",
    "new2 = []\n",
    "#looping through length of author column\n",
    "for i in range(len(authors)):\n",
    "    #turning string of list of dictionaries into list of dictionaries\n",
    "    temp = list(eval(authors.loc[i][0]))\n",
    "    #names\n",
    "    lis = []\n",
    "    #ids\n",
    "    lis2 = []\n",
    "    #looping through the list of dictionaries\n",
    "    for i in temp:\n",
    "        if 'first_name' in i:\n",
    "            first = i['first_name']\n",
    "            last = i['last_name']\n",
    "            #concatenating first and last name\n",
    "            full = first + \" \" + last\n",
    "            lis.append(full)\n",
    "            #print(lis)\n",
    "            ids = i['researcher_id']\n",
    "            lis2.append(ids)\n",
    "        else:\n",
    "            lis.append(i)\n",
    "            lis2.append(i)\n",
    "    new.append(lis)\n",
    "    new2.append(lis2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding new column, \"names,\" to the original dataframe\n",
    "names = pd.Series(new)\n",
    "df['names'] = names.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding new column, \"ids,\" to the original dataframe\n",
    "ids = pd.Series(new2)\n",
    "df['ids'] = ids.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate data by researcher-year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2 = df.explode(['names', 'ids']).reset_index(drop=True)\n",
    "df2 = df.apply(pd.Series.explode).reset_index(drop=True)\n",
    "\n",
    "testing = df2['ids'].value_counts()\n",
    "#print(testing.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdsi = pd.read_csv(\"HDSI.csv\")\n",
    "faculty = hdsi[hdsi['Dimensions ID'] != 'no ID']['Dimensions ID']\n",
    "#manually adding professors since they do not have dimensions ids\n",
    "add = pd.Series(['Aaron McMillan Fraenkel', 'Justin Eldridge'])\n",
    "faculty = list(faculty.append(add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned out all names & ids that do not match our hdsi faculty list\n",
    "df3 = df2[df2.ids.isin(faculty)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df3['ids'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['abstract'] = df3['abstract'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant = ['abstract', 'purpose', 'paper', 'goal', 'usepackage', 'cod']\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "def preprocess_abstract(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in redundant:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return \" \".join(result)\n",
    "\n",
    "\n",
    "df3['abstract_processed'] = df3['abstract'].apply(preprocess_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3[df3['year'] >= 2015]\n",
    "counts = CountVectorizer().fit_transform(df3['abstract_processed'])\n",
    "authors = {}\n",
    "for author in df3.names.unique():\n",
    "    authors[author] = {\n",
    "        2015 : list(),\n",
    "        2016 : list(),\n",
    "        2017 : list(),\n",
    "        2018 : list(),\n",
    "        2019 : list(),\n",
    "        2020 : list(),\n",
    "        2021 : list()\n",
    "    }\n",
    "for i, row in df3.iterrows():\n",
    "    authors[row['names']][row['year']].append(row['abstract_processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "381"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs = []\n",
    "missing_author_years = {author : list() for author in df3.names.unique()}\n",
    "for author, author_dict in authors.items():\n",
    "    for year, documents in author_dict.items():\n",
    "        if len(documents) == 0:\n",
    "            missing_author_years[author].append(year)\n",
    "            continue\n",
    "        all_docs.append(\" \".join(documents))\n",
    "len(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initate LDA model\n",
    "countVec = CountVectorizer()\n",
    "counts = countVec.fit_transform(all_docs)\n",
    "names = countVec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modeller = LatentDirichletAllocation(n_components=10, n_jobs=-1, random_state=123)\n",
    "result = modeller.fit_transform(counts)\n",
    "modeller2 = LatentDirichletAllocation(n_components=20, n_jobs=-1, random_state=123)\n",
    "result2 = modeller2.fit_transform(counts)\n",
    "modeller3 = LatentDirichletAllocation(n_components=30, n_jobs=-1, random_state=123)\n",
    "result3 = modeller3.fit_transform(counts)\n",
    "modeller4 = LatentDirichletAllocation(n_components=40, n_jobs=-1, random_state=123)\n",
    "result4 = modeller4.fit_transform(counts)\n",
    "modeller5 = LatentDirichletAllocation(n_components=50, n_jobs=-1, random_state=123)\n",
    "result5 = modeller5.fit_transform(counts)\n",
    "\n",
    "models = {'10':modeller,'20':modeller2,'30':modeller3,'40':modeller4,'50':modeller5}\n",
    "results = {'10':result,'20':result2,'30':result3,'40':result4,'50':result5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "topicnames = {\n",
    "    num_topics : [\"Topic\" + str(i) for i in range(num_topics)] for num_topics in range(10, 60, 10)\n",
    "}\n",
    "\n",
    "# index names\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(all_docs))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = {\n",
    "    num_topics : pd.DataFrame(results[f'{num_topics}'], columns=topicnames[num_topics], index=docnames) for num_topics in range(10, 60, 10)\n",
    "}\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = {\n",
    "    num_topics : np.argmax(df_document_topic[num_topics].values, axis=1) for num_topics in range(10, 60, 10)\n",
    "}\n",
    "\n",
    "for num_topics, df in df_document_topic.items():\n",
    "    df['dominant_topic'] = dominant_topic[num_topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_list = []\n",
    "year_list = []\n",
    "for author in authors.keys():\n",
    "    for i in range(7):\n",
    "        if (2015 + i) not in missing_author_years[author]:\n",
    "            author_list.append(author)\n",
    "            year_list.append(2015 + i)\n",
    "\n",
    "for df in df_document_topic.values():\n",
    "    df['author'] = author_list\n",
    "    df['year'] = year_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged = {\n",
    "    num_topics : df_document_topic[num_topics].groupby('author').mean().drop(['dominant_topic', 'year'], axis=1) for num_topics in df_document_topic.keys()\n",
    "}\n",
    "\n",
    "filtered = {\n",
    "    threshold : {num_topics : averaged[num_topics].mask(averaged[num_topics] < threshold, other=0) for num_topics in averaged.keys()} for threshold in [.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "for num_topics in range(10, 60, 10):\n",
    "    labels[num_topics] = filtered[.1][num_topics].index.to_list()\n",
    "    labels[num_topics].extend(filtered[.1][num_topics].columns.to_list())\n",
    "\n",
    "\n",
    "sources = {threshold : {} for threshold in [.1]}\n",
    "targets = {threshold : {} for threshold in [.1]}\n",
    "values = {threshold : {} for threshold in [.1]}\n",
    "\n",
    "for threshold in [.1]:\n",
    "    for num_topics in range(10, 60, 10):\n",
    "        curr_sources = []\n",
    "        curr_targets = []\n",
    "        curr_values = []\n",
    "        index_counter = 0\n",
    "        for index, row in filtered[threshold][num_topics].iterrows():\n",
    "            for i, value in enumerate(row):\n",
    "                if value != 0:\n",
    "                    curr_sources.append(index_counter)\n",
    "                    curr_targets.append(108 + i)\n",
    "                    curr_values.append(value)\n",
    "            index_counter += 1\n",
    "        sources[threshold][num_topics] = curr_sources\n",
    "        targets[threshold][num_topics] = curr_targets\n",
    "        values[threshold][num_topics] = curr_values\n",
    "\n",
    "positions = {\n",
    "    num_topics : {label : i for i, label in enumerate(labels[num_topics])} for num_topics in averaged.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_ranks(array):\n",
    "    ranks = []\n",
    "    for value in array:\n",
    "        for i, percentage in enumerate(np.arange(.1, 1.1, .1)):\n",
    "            if value <= np.quantile(array, percentage):\n",
    "                ranks.append(i + 1)\n",
    "                break\n",
    "    return ranks\n",
    "\n",
    "final_values = {threshold : {} for threshold in [.1]}\n",
    "\n",
    "for threshold in [.1]:\n",
    "    for num_topics in range(10, 60, 10):\n",
    "        curr_values_array = np.array(values[threshold][num_topics])\n",
    "        final_values[threshold][num_topics] = split_into_ranks(curr_values_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics_list(model, feature_names, no_top_words):\n",
    "    topic_list = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_list.append(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "    return topic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_labels = {}\n",
    "for num_topics in range(10, 60, 10):\n",
    "    link_labels[num_topics] = labels[num_topics].copy()\n",
    "    link_labels[num_topics][50:] = display_topics_list(models[f'{num_topics}'], names, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py:104: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counts = CountVectorizer().fit_transform(df3['abstract_processed'])\n",
    "transformed_list = []\n",
    "for model in models.values():\n",
    "    transformed_list.append(model.transform(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {threshold : {} for threshold in [.1]}\n",
    "for i, matrix in enumerate(transformed_list):\n",
    "    for threshold in [.1]:\n",
    "        df = pd.DataFrame(matrix)\n",
    "        df.mask(df < threshold, other=0, inplace=True)\n",
    "        df['author'] = df3['names']\n",
    "        df['year'] = df3['year']\n",
    "        df['citations'] = df3['times_cited'] + 1\n",
    "\n",
    "        # noralization of citations: Scaling to a range [0, 1]\n",
    "        df['citations_norm'] = df.groupby(by=['author', 'year'])['citations'].apply(lambda x: (x-x.min())/(x.max()-x.min()))#normalize_by_group(df=df, by=['author', 'year'])['citations']\n",
    "        df['abstract'] = df3['abstract']\n",
    "        df['title'] = df3['title']\n",
    "        df.fillna(1, inplace=True)\n",
    "        \n",
    "        #alpha weight parameter for weighting importance of citations vs topic relation\n",
    "        alpha = .75\n",
    "        for topic_num in range((i+1) * 10):\n",
    "            df[f'{topic_num}_relevance'] = alpha * df[topic_num] + (1-alpha) * df['citations_norm']\n",
    "        dataframes[threshold][(i+1) * 10] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_top_list(data_frame, num_topics, threshold):\n",
    "    top_5s = []\n",
    "    the_filter = filtered[threshold][num_topics]\n",
    "    for topic in range(num_topics):\n",
    "        relevant = the_filter[the_filter[f'Topic{topic}'] != 0].index.to_list()\n",
    "        to_append = data_frame[data_frame[f'{topic}_relevance'] > 0].reset_index()\n",
    "        to_append = to_append[to_append['author'].isin(relevant)].reset_index()\n",
    "        top_5s.append(to_append) \n",
    "    return top_5s\n",
    "\n",
    "tops = {\n",
    "    threshold : {num_topics : create_top_list(dataframes[threshold][num_topics], num_topics, threshold) for num_topics in range(10, 60, 10)} for threshold in [.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import dash_bootstrap_components as dbc\n",
    "import plotly.graph_objects as go\n",
    "from dash.dependencies import Input, Output, State\n",
    "\n",
    "# sankey diagrams for diff numbers of topics\n",
    "\n",
    "heights = {\n",
    "  10 : 1000,\n",
    "  20 : 1500,\n",
    "  30 : 2000,\n",
    "  40 : 2500,\n",
    "  50 : 3000\n",
    "}\n",
    "\n",
    "figs = {threshold : {} for threshold in [.1]}\n",
    "for threshold in [.1]:\n",
    "    for num_topics in range(10, 60, 10):\n",
    "        fig = go.Figure(data=[go.Sankey(\n",
    "            node = dict(\n",
    "                pad = 15,\n",
    "                thickness = 20,\n",
    "                line = dict(color = 'black', width = 0.5),\n",
    "                label = labels[num_topics],\n",
    "                color = ['#666699' for i in range(len(labels[num_topics]))],\n",
    "                customdata = link_labels[num_topics],\n",
    "                hovertemplate='%{customdata} Total Flow: %{value}<extra></extra>'\n",
    "            ),\n",
    "            link = dict(\n",
    "                color = ['rgba(204, 204, 204, .5)' for i in range(len(sources[threshold][num_topics]))],\n",
    "                source = sources[threshold][num_topics],\n",
    "                target = targets[threshold][num_topics],\n",
    "                value = final_values[threshold][num_topics]\n",
    "            )\n",
    "        )])\n",
    "        fig.update_layout(title_text=\"Author Topic Connections\", font=dict(size = 10, color = 'white'), height=heights[num_topics], paper_bgcolor=\"black\", plot_bgcolor='black')\n",
    "        figs[threshold][num_topics] = fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = {\n",
    "    10 : display_topics_list(models['10'], names, 10),\n",
    "    20 : display_topics_list(models['20'], names, 10),\n",
    "    30 : display_topics_list(models['30'], names, 10),\n",
    "    40 : display_topics_list(models['40'], names, 10),\n",
    "    50 : display_topics_list(models['50'], names, 10)\n",
    "}\n",
    "\n",
    "combined = pd.read_csv('final_hdsi_faculty_updated.csv')\n",
    "#combined[combined.title == 'Unperturbed: spectral analysis beyond Davis-Kahan'].abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = {}\n",
    "for i, word in enumerate(names):\n",
    "    locations[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:8050/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [05/Dec/2021 11:05:08] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [05/Dec/2021 11:05:08] \"GET /_dash-component-suites/dash/dcc/async-graph.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [05/Dec/2021 11:05:08] \"GET /_dash-component-suites/dash/dcc/async-dropdown.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [05/Dec/2021 11:05:08] \"GET /_dash-component-suites/dash/dcc/async-plotlyjs.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [05/Dec/2021 11:05:08] \"GET /_dash-layout HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [05/Dec/2021 11:05:08] \"GET /_dash-dependencies HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [05/Dec/2021 11:05:08] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [05/Dec/2021 11:05:10] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [05/Dec/2021 11:05:10] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [05/Dec/2021 11:05:13] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [05/Dec/2021 11:05:13] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [05/Dec/2021 11:05:13] \"POST /_dash-update-component HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "threshold = .1\n",
    "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.DARKLY])\n",
    "\n",
    "app.layout = html.Div([\n",
    "  dbc.Row([\n",
    "      dcc.Dropdown(\n",
    "        id='graph-dropdown',\n",
    "        placeholder='select number of LDA topics',\n",
    "        options=[{'label' : f'{i} Topic Model', 'value' : i} for i in range(10, 60, 10)],\n",
    "        style={\n",
    "          'color' : 'black',\n",
    "          'background-color' : '#666699',\n",
    "          'width' : '200%',\n",
    "          'align-items' : 'left',\n",
    "          'justify-content' : 'left',\n",
    "          'padding-left' : '15px'\n",
    "        },\n",
    "        value=10\n",
    "      )\n",
    "  ]),\n",
    "  dbc.Row([\n",
    "    dbc.Col(html.Div([\n",
    "      dcc.Graph(\n",
    "        id = 'graph',\n",
    "        figure = figs[.1][10]\n",
    "      )\n",
    "      ],\n",
    "      style={\n",
    "        'height' : '100vh',\n",
    "        'overflow-y' : 'scroll'\n",
    "      }\n",
    "    )\n",
    "    ),\n",
    "      dbc.Col(html.Div([dbc.Col([\n",
    "        dcc.Dropdown(\n",
    "          id='dropdown_menu',\n",
    "          placeholder='Select a topic',\n",
    "          options=[{'label' : f'Topic {topic}: {top_words[10][topic]}', 'value' : topic} for topic in range(10)],\n",
    "          style={\n",
    "            'color' : 'black',\n",
    "            'background-color' : 'white'\n",
    "          }\n",
    "        ),\n",
    "        dcc.Dropdown(\n",
    "          id='researcher-dropdown',\n",
    "          placeholder='Select Researchers',\n",
    "          options=[{'label' : f'{researcher}', 'value' : f'{researcher}'} for researcher in set(author_list)],\n",
    "          style={\n",
    "            'color' : 'black',\n",
    "            'background-color' : 'white'\n",
    "          }\n",
    "        )]),\n",
    "        dbc.Col(\n",
    "          dcc.Dropdown(\n",
    "            id='word-search',\n",
    "            placeholder='Search by word',\n",
    "            options=[{'label' : word, 'value' : word} for word in names],\n",
    "            style={\n",
    "              'color' : 'black',\n",
    "              'background-color' : 'white'\n",
    "            },\n",
    "            value=[],\n",
    "            multi=True\n",
    "          )\n",
    "        ),\n",
    "        html.Div(\n",
    "          id='paper_container', \n",
    "          children=[\n",
    "            html.P(\n",
    "              children=['Top 5 Papers'],\n",
    "              id='titles_and_authors', \n",
    "              draggable=False, \n",
    "              style={\n",
    "                'font-size' :'150%',\n",
    "                'font-family' : 'Verdana'\n",
    "              }\n",
    "            ),\n",
    "          ],\n",
    "        ),\n",
    "      ], \n",
    "        style={\n",
    "          'height' : '100vh',\n",
    "          'overflow-y' : 'scroll'\n",
    "        }\n",
    "      )\n",
    "      )\n",
    "    ]\n",
    "  )]\n",
    ")\n",
    "\n",
    "@app.callback(\n",
    "  Output('titles_and_authors', 'children'),\n",
    "  Output('researcher-dropdown', 'options'),\n",
    "  Input('dropdown_menu', 'value'),\n",
    "  Input('graph-dropdown', 'value'),\n",
    "  Input('researcher-dropdown', 'value'),\n",
    "  Input('word-search', 'value')\n",
    ")\n",
    "def update_p(topic, num_topics, author, words):\n",
    "  if len(words) != 0:\n",
    "    doc_vec = np.zeros((1, len(names)))\n",
    "    for word in words:\n",
    "      doc_vec[0][locations[word]] += 1\n",
    "    relations = np.round(models[f'{num_topics}'].transform(doc_vec), 5).tolist()[0]\n",
    "    pairs = [(i, relation) for i, relation in enumerate(relations)]\n",
    "    pairs.sort(reverse=True, key=lambda x: x[1])\n",
    "    to_return = [[html.Br(), f'Topic{pair[0]}: {pair[1]}', html.Br()] for pair in pairs]\n",
    "    return list(chain(*to_return)), [{'label' : f'{researcher}', 'value' : f'{researcher}'} for researcher in set(author_list)]\n",
    "\n",
    "  if topic == None and author == None:\n",
    "    return ['Make a selection'], [{'label' : f'{researcher}', 'value' : f'{researcher}'} for researcher in set(author_list)]\n",
    "\n",
    "  if topic != None and author == None:\n",
    "    df = tops[threshold][num_topics][topic]\n",
    "    df_authors = df.author.unique()\n",
    "    max_vals = df.groupby('author').max()[f'{topic}_relevance']\n",
    "\n",
    "    to_return = [[f'{name}:', html.Br(), \n",
    "      f'{df[df[f\"{topic}_relevance\"] == max_vals.loc[name]][\"title\"].to_list()[0]}',\n",
    "      html.Details([html.Summary('Abstract'),\n",
    "                    html.Div(combined[combined.title == f'{df[df[f\"{topic}_relevance\"] == max_vals.loc[name]][\"title\"].to_list()[0]}'].abstract)],\n",
    "                    style={\n",
    "                      'font-size' :'80%',\n",
    "                      'font-family' : 'Verdana'}),\n",
    "      html.Br()] for i, name in enumerate(max_vals.index)]\n",
    "    return list(chain(*to_return)), [{'label' : f'{researcher}', 'value' : f'{researcher}'} for researcher in tops[threshold][num_topics][topic].author.unique()]\n",
    "\n",
    "  if topic == None and author != None:\n",
    "    to_return = []\n",
    "    for topic_num in range(num_topics):\n",
    "      df = tops[threshold][num_topics][topic_num]\n",
    "      if author in df.author.unique():\n",
    "        max_vals = df.groupby('author').max()[f'{topic_num}_relevance']\n",
    "  \n",
    "        to_return.append([f'Topic {topic_num}:', html.Br(), \n",
    "          f'{df[df[f\"{topic_num}_relevance\"] == max_vals.loc[author]][\"title\"].to_list()[0]}', \n",
    "          html.Details([html.Summary('Abstract'), \n",
    "                        html.Div(combined[combined.title == f'{df[df[f\"{topic_num}_relevance\"] == max_vals.loc[author]][\"title\"].to_list()[0]}'].abstract)],\n",
    "                        style={\n",
    "                          'font-size' :'80%',\n",
    "                          'font-family' : 'Verdana'},\n",
    "                        ),\n",
    "          html.Br()])\n",
    "    return list(chain(*to_return)), [{'label' : f'{researcher}', 'value' : f'{researcher}'} for researcher in set(author_list)]\n",
    "\n",
    "  if topic != None and author != None:\n",
    "    df = tops[threshold][num_topics][topic]\n",
    "    df = df[df['author'] == author]\n",
    "    df.sort_values(by=f'{topic}_relevance', ascending=False, inplace=True)\n",
    "    titles = df.head(10)['title'].to_list()\n",
    "    \n",
    "    to_return = [\n",
    "      [f'{i} : {title}', \n",
    "      html.Details([html.Summary('Abstract'), \n",
    "                    html.Div(combined[combined.title == title].abstract)], \n",
    "                    style={\n",
    "                      'font-size' :'80%',\n",
    "                      'font-family' : 'Verdana'}), \n",
    "      html.Br()] for i, title in enumerate(titles)]\n",
    "    return list(chain(*to_return)), [{'label' : f'{researcher}', 'value' : f'{researcher}'} for researcher in tops[threshold][num_topics][topic].author.unique()]\n",
    "    \n",
    "\n",
    "\n",
    "@app.callback(\n",
    "  [Output('graph', 'figure'), Output('dropdown_menu', 'options')],\n",
    "  [Input('graph-dropdown', 'value'), Input('dropdown_menu', 'value'), Input('researcher-dropdown', 'value'), Input('word-search', 'value')],\n",
    "  State('graph', 'figure')\n",
    ")\n",
    "def update_graph(value, topic, author, words, previous_fig):\n",
    "  if len(previous_fig['data'][0]['node']['color']) != value + 108:\n",
    "    figs[threshold][value].update_traces(node = dict(color = ['#666699' for i in range(len(labels[value]))]), link = dict(color = ['rgba(204, 204, 204, .5)' for i in range(len(sources[threshold][value]))]))\n",
    "    return figs[threshold][value], [{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "\n",
    "  if len(words) != 0:\n",
    "    doc_vec = np.zeros((1, len(names)))\n",
    "    for word in words:\n",
    "      doc_vec[0][locations[word]] += 1\n",
    "    relations = np.round(models[f'{value}'].transform(doc_vec), 3).tolist()[0]\n",
    "    opacity = {(i+108) : relation for i, relation in enumerate(relations) if relation > .1}\n",
    "    node_colors = ['#666699' if (i not in opacity.keys()) else f'rgba(255, 255, 0, {opacity[i]})' for i in range(len(labels[value]))]\n",
    "    valid_targets = [positions[value][f'Topic{i-108}'] for i in opacity.keys()]\n",
    "    link_colors = ['rgba(204, 204, 204, .5)' if target not in valid_targets else f'rgba(255, 255, 0, .5)' for target in targets[threshold][value]]\n",
    "    figs[threshold][value].update_traces(node = dict(color = node_colors), link = dict(color = link_colors)),\n",
    "    return figs[threshold][value], [{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "\n",
    "\n",
    "  if topic == None and author == None:\n",
    "    figs[threshold][value].update_traces(node = dict(color = ['#666699' for i in range(len(labels[value]))]), link = dict(color = ['rgba(204, 204, 204, .5)' for i in range(len(sources[threshold][value]))]))\n",
    "    return figs[threshold][value], [{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "  \n",
    "  if topic != None and author == None:\n",
    "    node_colors = ['#666699' if (i != positions[value][f'Topic{topic}']) else '#ffff00' for i in range(len(labels[value]))]\n",
    "    link_colors = ['rgba(204, 204, 204, .5)' if target != positions[value][f'Topic{topic}'] else 'rgba(255, 255, 0, .5)' for target in targets[threshold][value]]\n",
    "    figs[threshold][value].update_traces(node = dict(color = node_colors), link = dict(color = link_colors))\n",
    "    return figs[threshold][value], [{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "\n",
    "  if topic == None and author != None:\n",
    "    node_colors = ['#666699' if (i != positions[value][author]) else '#ffff00' for i in range(len(labels[value]))]\n",
    "    link_colors = ['rgba(204, 204, 204, .5)' if source != positions[value][author] else 'rgba(255, 255, 0, .5)' for source in sources[threshold][value]]\n",
    "    figs[threshold][value].update_traces(node = dict(color = node_colors), link = dict(color = link_colors))\n",
    "    return figs[threshold][value], [{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "\n",
    "  if topic != None and author != None:\n",
    "    node_colors = ['#666699' if (i != positions[value][author] and i != positions[value][f'Topic{topic}']) else '#ffff00' for i in range(len(labels[value]))]\n",
    "    link_colors = ['rgba(204, 204, 204, .5)' if (source != positions[value][author] or target != positions[value][f'Topic{topic}']) else 'rgba(255, 255, 0, .5)' for source, target in zip(sources[threshold][value], targets[threshold][value])]\n",
    "    figs[threshold][value].update_traces(node = dict(color = node_colors), link = dict(color = link_colors))\n",
    "    return figs[threshold][value], [{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "\n",
    "@app.callback(\n",
    "  Output('researcher-dropdown', 'value'),\n",
    "  Input('dropdown_menu', 'value'),\n",
    "  State('dropdown_menu', 'value')\n",
    ")\n",
    "def reset_author(topic, previous):\n",
    "  if topic != previous:\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "app.run_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
